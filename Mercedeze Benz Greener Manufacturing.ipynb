{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af20390f",
   "metadata": {},
   "source": [
    "# Mercedes-Benz Greener Manufacturing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d518aa3",
   "metadata": {},
   "source": [
    "### DESCRIPTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9271d44a",
   "metadata": {},
   "source": [
    "#### Reduce the time a Mercedes-Benz spends on the test bench.\n",
    "\n",
    "Problem Statement Scenario:\n",
    "Since the first automobile, the Benz Patent Motor Car in 1886, Mercedes-Benz has stood for important automotive innovations. These include the passenger safety cell with a crumple zone, the airbag, and intelligent assistance systems. Mercedes-Benz applies for nearly 2000 patents per year, making the brand the European leader among premium carmakers. Mercedes-Benz is the leader in the premium car industry. With a huge selection of features and options, customers can choose the customized Mercedes-Benz of their dreams.\n",
    "\n",
    "To ensure the safety and reliability of every unique car configuration before they hit the road, the company’s engineers have developed a robust testing system. As one of the world’s biggest manufacturers of premium cars, safety and efficiency are paramount on Mercedes-Benz’s production lines. However, optimizing the speed of their testing system for many possible feature combinations is complex and time-consuming without a powerful algorithmic approach.\n",
    "\n",
    "You are required to reduce the time that cars spend on the test bench. Others will work with a dataset representing different permutations of features in a Mercedes-Benz car to predict the time it takes to pass testing. Optimal algorithms will contribute to faster testing, resulting in lower carbon dioxide emissions without reducing Mercedes-Benz’s standards.\n",
    "\n",
    "Following actions should be performed:\n",
    "\n",
    "    1.If for any column(s),the variance is equal to zero, then you need to remove those variable(s).\n",
    "    2.Check for null and unique values for test and train sets.\n",
    "    3.Apply label encoder.\n",
    "    4.Perform dimensionality reduction.\n",
    "    5.Predict your test_df values using XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "601074fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Info : \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4209 entries, 0 to 4208\n",
      "Columns: 378 entries, ID to X385\n",
      "dtypes: float64(1), int64(369), object(8)\n",
      "memory usage: 12.1+ MB\n",
      "None \n",
      "\n",
      "\n",
      "\n",
      "Training Data : \n",
      "\n",
      "   ID       y  X0 X1  X2 X3 X4 X5 X6 X8  ...  X375  X376  X377  X378  X379  \\\n",
      "0   0  130.81   k  v  at  a  d  u  j  o  ...     0     0     1     0     0   \n",
      "1   6   88.53   k  t  av  e  d  y  l  o  ...     1     0     0     0     0   \n",
      "2   7   76.26  az  w   n  c  d  x  j  x  ...     0     0     0     0     0   \n",
      "3   9   80.62  az  t   n  f  d  x  l  e  ...     0     0     0     0     0   \n",
      "4  13   78.02  az  v   n  f  d  h  d  n  ...     0     0     0     0     0   \n",
      "\n",
      "   X380  X382  X383  X384  X385  \n",
      "0     0     0     0     0     0  \n",
      "1     0     0     0     0     0  \n",
      "2     0     1     0     0     0  \n",
      "3     0     0     0     0     0  \n",
      "4     0     0     0     0     0  \n",
      "\n",
      "[5 rows x 378 columns] \n",
      "\n",
      "\n",
      "\n",
      "Test Data : \n",
      "\n",
      "   ID  X0 X1  X2 X3 X4 X5 X6 X8  X10  ...  X375  X376  X377  X378  X379  X380  \\\n",
      "0   1  az  v   n  f  d  t  a  w    0  ...     0     0     0     1     0     0   \n",
      "1   2   t  b  ai  a  d  b  g  y    0  ...     0     0     1     0     0     0   \n",
      "2   3  az  v  as  f  d  a  j  j    0  ...     0     0     0     1     0     0   \n",
      "3   4  az  l   n  f  d  z  l  n    0  ...     0     0     0     1     0     0   \n",
      "4   5   w  s  as  c  d  y  i  m    0  ...     1     0     0     0     0     0   \n",
      "\n",
      "   X382  X383  X384  X385  \n",
      "0     0     0     0     0  \n",
      "1     0     0     0     0  \n",
      "2     0     0     0     0  \n",
      "3     0     0     0     0  \n",
      "4     0     0     0     0  \n",
      "\n",
      "[5 rows x 377 columns] \n",
      "\n",
      "\n",
      "\n",
      "Train Dataframe shape : \n",
      "\n",
      "(4209, 378) \n",
      "\n",
      "\n",
      "\n",
      "Test Dataframe shape : \n",
      "\n",
      "(4209, 377) \n",
      "\n",
      "\n",
      "\n",
      "Train Dataframe after dropping ID and y columns\n",
      "\n",
      "   X0 X1  X2 X3 X4 X5 X6 X8  X10  X11  ...  X375  X376  X377  X378  X379  \\\n",
      "0   k  v  at  a  d  u  j  o    0    0  ...     0     0     1     0     0   \n",
      "1   k  t  av  e  d  y  l  o    0    0  ...     1     0     0     0     0   \n",
      "2  az  w   n  c  d  x  j  x    0    0  ...     0     0     0     0     0   \n",
      "3  az  t   n  f  d  x  l  e    0    0  ...     0     0     0     0     0   \n",
      "4  az  v   n  f  d  h  d  n    0    0  ...     0     0     0     0     0   \n",
      "\n",
      "   X380  X382  X383  X384  X385  \n",
      "0     0     0     0     0     0  \n",
      "1     0     0     0     0     0  \n",
      "2     0     1     0     0     0  \n",
      "3     0     0     0     0     0  \n",
      "4     0     0     0     0     0  \n",
      "\n",
      "[5 rows x 376 columns] \n",
      "\n",
      "\n",
      "\n",
      "Test dataframe after dropping ID column\n",
      "\n",
      "   X0 X1  X2 X3 X4 X5 X6 X8  X10  X11  ...  X375  X376  X377  X378  X379  \\\n",
      "0  az  v   n  f  d  t  a  w    0    0  ...     0     0     0     1     0   \n",
      "1   t  b  ai  a  d  b  g  y    0    0  ...     0     0     1     0     0   \n",
      "2  az  v  as  f  d  a  j  j    0    0  ...     0     0     0     1     0   \n",
      "3  az  l   n  f  d  z  l  n    0    0  ...     0     0     0     1     0   \n",
      "4   w  s  as  c  d  y  i  m    0    0  ...     1     0     0     0     0   \n",
      "\n",
      "   X380  X382  X383  X384  X385  \n",
      "0     0     0     0     0     0  \n",
      "1     0     0     0     0     0  \n",
      "2     0     0     0     0     0  \n",
      "3     0     0     0     0     0  \n",
      "4     0     0     0     0     0  \n",
      "\n",
      "[5 rows x 376 columns] \n",
      "\n",
      "\n",
      "\n",
      "Train Dataframe after removing columns with varaince 0 : \n",
      "\n",
      "   X0 X1  X2 X3 X4 X5 X6 X8  X10  X12  ...  X375  X376  X377  X378  X379  \\\n",
      "0   k  v  at  a  d  u  j  o    0    0  ...     0     0     1     0     0   \n",
      "1   k  t  av  e  d  y  l  o    0    0  ...     1     0     0     0     0   \n",
      "2  az  w   n  c  d  x  j  x    0    0  ...     0     0     0     0     0   \n",
      "3  az  t   n  f  d  x  l  e    0    0  ...     0     0     0     0     0   \n",
      "4  az  v   n  f  d  h  d  n    0    0  ...     0     0     0     0     0   \n",
      "\n",
      "   X380  X382  X383  X384  X385  \n",
      "0     0     0     0     0     0  \n",
      "1     0     0     0     0     0  \n",
      "2     0     1     0     0     0  \n",
      "3     0     0     0     0     0  \n",
      "4     0     0     0     0     0  \n",
      "\n",
      "[5 rows x 364 columns] \n",
      "\n",
      "\n",
      "\n",
      "Test Dataframe after removing columns with varaince 0 : \n",
      "\n",
      "   X0 X1  X2 X3 X4 X5 X6 X8  X10  X12  ...  X375  X376  X377  X378  X379  \\\n",
      "0  az  v   n  f  d  t  a  w    0    0  ...     0     0     0     1     0   \n",
      "1   t  b  ai  a  d  b  g  y    0    0  ...     0     0     1     0     0   \n",
      "2  az  v  as  f  d  a  j  j    0    0  ...     0     0     0     1     0   \n",
      "3  az  l   n  f  d  z  l  n    0    0  ...     0     0     0     1     0   \n",
      "4   w  s  as  c  d  y  i  m    0    0  ...     1     0     0     0     0   \n",
      "\n",
      "   X380  X382  X383  X384  X385  \n",
      "0     0     0     0     0     0  \n",
      "1     0     0     0     0     0  \n",
      "2     0     0     0     0     0  \n",
      "3     0     0     0     0     0  \n",
      "4     0     0     0     0     0  \n",
      "\n",
      "[5 rows x 364 columns] \n",
      "\n",
      "\n",
      "\n",
      "Dataframe after concatinating train_df and test_df\n",
      "\n",
      "      X0  X1  X2 X3 X4  X5 X6 X8  X10  X12  ...  X375  X376  X377  X378  X379  \\\n",
      "0      k   v  at  a  d   u  j  o    0    0  ...     0     0     1     0     0   \n",
      "1      k   t  av  e  d   y  l  o    0    0  ...     1     0     0     0     0   \n",
      "2     az   w   n  c  d   x  j  x    0    0  ...     0     0     0     0     0   \n",
      "3     az   t   n  f  d   x  l  e    0    0  ...     0     0     0     0     0   \n",
      "4     az   v   n  f  d   h  d  n    0    0  ...     0     0     0     0     0   \n",
      "...   ..  ..  .. .. ..  .. .. ..  ...  ...  ...   ...   ...   ...   ...   ...   \n",
      "4204  aj   h  as  f  d  aa  j  e    0    0  ...     0     0     0     0     0   \n",
      "4205   t  aa  ai  d  d  aa  j  y    0    0  ...     0     1     0     0     0   \n",
      "4206   y   v  as  f  d  aa  d  w    0    0  ...     0     0     0     0     0   \n",
      "4207  ak   v  as  a  d  aa  c  q    0    0  ...     0     0     1     0     0   \n",
      "4208   t  aa  ai  c  d  aa  g  r    0    0  ...     1     0     0     0     0   \n",
      "\n",
      "      X380  X382  X383  X384  X385  \n",
      "0        0     0     0     0     0  \n",
      "1        0     0     0     0     0  \n",
      "2        0     1     0     0     0  \n",
      "3        0     0     0     0     0  \n",
      "4        0     0     0     0     0  \n",
      "...    ...   ...   ...   ...   ...  \n",
      "4204     0     0     0     0     0  \n",
      "4205     0     0     0     0     0  \n",
      "4206     0     0     0     0     0  \n",
      "4207     0     0     0     0     0  \n",
      "4208     0     0     0     0     0  \n",
      "\n",
      "[8418 rows x 364 columns] \n",
      "\n",
      "\n",
      "\n",
      "Check null values for train sets. \n",
      "\n",
      "Index([], dtype='object')\n",
      "Empty DataFrame\n",
      "Columns: [X0, X1, X2, X3, X4, X5, X6, X8, X10, X12, X13, X14, X15, X16, X17, X18, X19, X20, X21, X22, X23, X24, X26, X27, X28, X29, X30, X31, X32, X33, X34, X35, X36, X37, X38, X39, X40, X41, X42, X43, X44, X45, X46, X47, X48, X49, X50, X51, X52, X53, X54, X55, X56, X57, X58, X59, X60, X61, X62, X63, X64, X65, X66, X67, X68, X69, X70, X71, X73, X74, X75, X76, X77, X78, X79, X80, X81, X82, X83, X84, X85, X86, X87, X88, X89, X90, X91, X92, X94, X95, X96, X97, X98, X99, X100, X101, X102, X103, X104, X105, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 364 columns] \n",
      "\n",
      "\n",
      "\n",
      "Check null values for test sets.\n",
      "\n",
      "Index([], dtype='object')\n",
      "Empty DataFrame\n",
      "Columns: [X0, X1, X2, X3, X4, X5, X6, X8, X10, X12, X13, X14, X15, X16, X17, X18, X19, X20, X21, X22, X23, X24, X26, X27, X28, X29, X30, X31, X32, X33, X34, X35, X36, X37, X38, X39, X40, X41, X42, X43, X44, X45, X46, X47, X48, X49, X50, X51, X52, X53, X54, X55, X56, X57, X58, X59, X60, X61, X62, X63, X64, X65, X66, X67, X68, X69, X70, X71, X73, X74, X75, X76, X77, X78, X79, X80, X81, X82, X83, X84, X85, X86, X87, X88, X89, X90, X91, X92, X94, X95, X96, X97, X98, X99, X100, X101, X102, X103, X104, X105, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 364 columns] \n",
      "\n",
      "\n",
      "\n",
      "Total Dataframe after applying Label Encoder : \n",
      "\n",
      "      X0  X1  X2  X3  X4  X5  X6  X8  X10  X12  ...  X375  X376  X377  X378  \\\n",
      "0     37  23  20   0   3  27   9  14    0    0  ...     0     0     1     0   \n",
      "1     37  21  22   4   3  31  11  14    0    0  ...     1     0     0     0   \n",
      "2     24  24  38   2   3  30   9  23    0    0  ...     0     0     0     0   \n",
      "3     24  21  38   5   3  30  11   4    0    0  ...     0     0     0     0   \n",
      "4     24  23  38   5   3  14   3  13    0    0  ...     0     0     0     0   \n",
      "...   ..  ..  ..  ..  ..  ..  ..  ..  ...  ...  ...   ...   ...   ...   ...   \n",
      "4204   9   9  19   5   3   1   9   4    0    0  ...     0     0     0     0   \n",
      "4205  46   1   9   3   3   1   9  24    0    0  ...     0     1     0     0   \n",
      "4206  51  23  19   5   3   1   3  22    0    0  ...     0     0     0     0   \n",
      "4207  10  23  19   0   3   1   2  16    0    0  ...     0     0     1     0   \n",
      "4208  46   1   9   2   3   1   6  17    0    0  ...     1     0     0     0   \n",
      "\n",
      "      X379  X380  X382  X383  X384  X385  \n",
      "0        0     0     0     0     0     0  \n",
      "1        0     0     0     0     0     0  \n",
      "2        0     0     1     0     0     0  \n",
      "3        0     0     0     0     0     0  \n",
      "4        0     0     0     0     0     0  \n",
      "...    ...   ...   ...   ...   ...   ...  \n",
      "4204     0     0     0     0     0     0  \n",
      "4205     0     0     0     0     0     0  \n",
      "4206     0     0     0     0     0     0  \n",
      "4207     0     0     0     0     0     0  \n",
      "4208     0     0     0     0     0     0  \n",
      "\n",
      "[8418 rows x 364 columns] \n",
      "\n",
      "\n",
      "\n",
      "Train Dataframe after applying Label Encoder\n",
      "\n",
      "      X0  X1  X2  X3  X4  X5  X6  X8  X10  X12  ...  X375  X376  X377  X378  \\\n",
      "0     37  23  20   0   3  27   9  14    0    0  ...     0     0     1     0   \n",
      "1     37  21  22   4   3  31  11  14    0    0  ...     1     0     0     0   \n",
      "2     24  24  38   2   3  30   9  23    0    0  ...     0     0     0     0   \n",
      "3     24  21  38   5   3  30  11   4    0    0  ...     0     0     0     0   \n",
      "4     24  23  38   5   3  14   3  13    0    0  ...     0     0     0     0   \n",
      "...   ..  ..  ..  ..  ..  ..  ..  ..  ...  ...  ...   ...   ...   ...   ...   \n",
      "4204  10  20  19   2   3   1   3  16    0    0  ...     1     0     0     0   \n",
      "4205  36  16  44   3   3   1   7   7    0    0  ...     0     1     0     0   \n",
      "4206  10  23  42   0   3   1   6   4    0    1  ...     0     0     1     0   \n",
      "4207  11  19  29   5   3   1  11  20    0    0  ...     0     0     0     0   \n",
      "4208  52  19   5   2   3   1   6  22    0    0  ...     1     0     0     0   \n",
      "\n",
      "      X379  X380  X382  X383  X384  X385  \n",
      "0        0     0     0     0     0     0  \n",
      "1        0     0     0     0     0     0  \n",
      "2        0     0     1     0     0     0  \n",
      "3        0     0     0     0     0     0  \n",
      "4        0     0     0     0     0     0  \n",
      "...    ...   ...   ...   ...   ...   ...  \n",
      "4204     0     0     0     0     0     0  \n",
      "4205     0     0     0     0     0     0  \n",
      "4206     0     0     0     0     0     0  \n",
      "4207     0     0     0     0     0     0  \n",
      "4208     0     0     0     0     0     0  \n",
      "\n",
      "[4209 rows x 364 columns] \n",
      "\n",
      "\n",
      "\n",
      "Test Dataframe after applying Label Encoder\n",
      "\n",
      "      X0  X1  X2  X3  X4  X5  X6  X8  X10  X12  ...  X375  X376  X377  X378  \\\n",
      "0     24  23  38   5   3  26   0  22    0    0  ...     0     0     0     1   \n",
      "1     46   3   9   0   3   9   6  24    0    0  ...     0     0     1     0   \n",
      "2     24  23  19   5   3   0   9   9    0    0  ...     0     0     0     1   \n",
      "3     24  13  38   5   3  32  11  13    0    0  ...     0     0     0     1   \n",
      "4     49  20  19   2   3  31   8  12    0    0  ...     1     0     0     0   \n",
      "...   ..  ..  ..  ..  ..  ..  ..  ..  ...  ...  ...   ...   ...   ...   ...   \n",
      "4204   9   9  19   5   3   1   9   4    0    0  ...     0     0     0     0   \n",
      "4205  46   1   9   3   3   1   9  24    0    0  ...     0     1     0     0   \n",
      "4206  51  23  19   5   3   1   3  22    0    0  ...     0     0     0     0   \n",
      "4207  10  23  19   0   3   1   2  16    0    0  ...     0     0     1     0   \n",
      "4208  46   1   9   2   3   1   6  17    0    0  ...     1     0     0     0   \n",
      "\n",
      "      X379  X380  X382  X383  X384  X385  \n",
      "0        0     0     0     0     0     0  \n",
      "1        0     0     0     0     0     0  \n",
      "2        0     0     0     0     0     0  \n",
      "3        0     0     0     0     0     0  \n",
      "4        0     0     0     0     0     0  \n",
      "...    ...   ...   ...   ...   ...   ...  \n",
      "4204     0     0     0     0     0     0  \n",
      "4205     0     0     0     0     0     0  \n",
      "4206     0     0     0     0     0     0  \n",
      "4207     0     0     0     0     0     0  \n",
      "4208     0     0     0     0     0     0  \n",
      "\n",
      "[4209 rows x 364 columns] \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Dataset with Standard Scaler : \n",
      "\n",
      "(8418, 364) \n",
      "\n",
      "\n",
      "\n",
      "Explained variance ratio of Scaled Dataset \n",
      "\n",
      "[6.96235499e-02 5.61572698e-02 4.63808371e-02 3.43122001e-02\n",
      " 3.24041302e-02 3.14465839e-02 2.82590702e-02 2.11608931e-02\n",
      " 1.92092270e-02 1.73092528e-02 1.63288118e-02 1.59575381e-02\n",
      " 1.46133192e-02 1.38858481e-02 1.32424174e-02 1.23871474e-02\n",
      " 1.23418785e-02 1.15384889e-02 1.12730650e-02 1.08665066e-02\n",
      " 9.97892723e-03 9.32495675e-03 9.31301192e-03 9.04861164e-03\n",
      " 8.64015566e-03 8.12361434e-03 7.38541054e-03 7.37046756e-03\n",
      " 7.30015139e-03 6.96827188e-03 6.69562827e-03 6.62245755e-03\n",
      " 6.43082374e-03 6.26133328e-03 6.15169810e-03 6.06887290e-03\n",
      " 5.88604306e-03 5.76172595e-03 5.59895518e-03 5.53115304e-03\n",
      " 5.38447643e-03 5.32403293e-03 5.16816623e-03 5.13032768e-03\n",
      " 5.03658780e-03 4.95440550e-03 4.88788816e-03 4.79668755e-03\n",
      " 4.59349607e-03 4.57063671e-03 4.40552332e-03 4.32515874e-03\n",
      " 4.28000521e-03 4.19301704e-03 4.18183215e-03 4.06874556e-03\n",
      " 3.99262160e-03 3.90760865e-03 3.83368623e-03 3.77491969e-03\n",
      " 3.71427050e-03 3.64448633e-03 3.63041471e-03 3.57464166e-03\n",
      " 3.56456358e-03 3.53109322e-03 3.47491336e-03 3.38984137e-03\n",
      " 3.33094673e-03 3.32146308e-03 3.28570562e-03 3.22991664e-03\n",
      " 3.19952681e-03 3.17175719e-03 3.14589357e-03 3.10605133e-03\n",
      " 3.07500715e-03 3.05208371e-03 3.03849715e-03 3.00513552e-03\n",
      " 2.99402924e-03 2.96092763e-03 2.95390537e-03 2.92977461e-03\n",
      " 2.91643530e-03 2.89377938e-03 2.88055565e-03 2.86761340e-03\n",
      " 2.82919410e-03 2.78190012e-03 2.77563980e-03 2.75545762e-03\n",
      " 2.75013415e-03 2.73278325e-03 2.68975639e-03 2.68223333e-03\n",
      " 2.66850432e-03 2.64746406e-03 2.63614619e-03 2.59600396e-03\n",
      " 2.58830928e-03 2.55712759e-03 2.54941569e-03 2.50976028e-03\n",
      " 2.47657197e-03 2.46231766e-03 2.45742580e-03 2.43488478e-03\n",
      " 2.42447719e-03 2.39752859e-03 2.37958813e-03 2.36777150e-03\n",
      " 2.35078193e-03 2.31169412e-03 2.29336681e-03 2.26464931e-03\n",
      " 2.24021740e-03 2.21606786e-03 2.20458822e-03 2.18412167e-03\n",
      " 2.16272146e-03 2.11402513e-03 2.08718339e-03 2.06362892e-03\n",
      " 2.06135588e-03 2.04618997e-03 2.02593478e-03 1.99781850e-03\n",
      " 1.97719476e-03 1.96217648e-03 1.93994495e-03 1.92070943e-03\n",
      " 1.87756320e-03 1.86048339e-03 1.81450494e-03 1.80131232e-03\n",
      " 1.76394295e-03 1.75652490e-03 1.73406016e-03 1.72745110e-03\n",
      " 1.65874106e-03 1.65349250e-03 1.63495356e-03 1.59279342e-03\n",
      " 1.56501483e-03 1.54417159e-03 1.52019038e-03 1.43105348e-03\n",
      " 1.40700655e-03 1.39700477e-03 1.34805337e-03 1.32395968e-03\n",
      " 1.28558622e-03 1.26614200e-03 1.22896724e-03 1.20455692e-03\n",
      " 1.20344745e-03 1.18300056e-03 1.15591401e-03 1.13627471e-03\n",
      " 1.12465250e-03 1.09221194e-03 1.06983205e-03 1.03305377e-03\n",
      " 1.00207385e-03 9.80417090e-04 9.69559327e-04 9.27223959e-04\n",
      " 9.12933907e-04 9.06435284e-04 8.86633557e-04 8.73582939e-04\n",
      " 8.57698756e-04 8.19898545e-04 8.08761553e-04 7.92571657e-04\n",
      " 7.73508099e-04 7.63411873e-04 7.39213417e-04 7.13134881e-04\n",
      " 6.87613749e-04 6.77884483e-04 6.56780860e-04 6.24995633e-04\n",
      " 6.14738849e-04 5.88557669e-04 5.80413471e-04 5.69705930e-04\n",
      " 5.53520578e-04 5.37192237e-04 5.28439783e-04 5.16202030e-04\n",
      " 5.01959337e-04 4.85211238e-04 4.81888023e-04 4.78501555e-04\n",
      " 4.65410459e-04 4.50461737e-04 4.40943564e-04 4.27690560e-04\n",
      " 4.09305779e-04 4.03158117e-04 3.96284300e-04 3.92060420e-04\n",
      " 3.88554935e-04 3.68997813e-04 3.58577580e-04 3.46033818e-04\n",
      " 3.41817417e-04 3.37339046e-04 3.25933880e-04 3.14874990e-04\n",
      " 3.00233194e-04 2.79439915e-04 2.75502974e-04 2.69475169e-04\n",
      " 2.59940220e-04 2.56351983e-04 2.52869463e-04 2.44474512e-04\n",
      " 2.32658389e-04 2.13488944e-04 2.03995153e-04 1.97580022e-04\n",
      " 1.86373781e-04 1.78047363e-04 1.73043641e-04 1.69208880e-04\n",
      " 1.64915744e-04 1.60788065e-04 1.45396142e-04 1.41595309e-04\n",
      " 1.34537685e-04 1.26075396e-04 1.19202231e-04 1.13019004e-04\n",
      " 1.05674283e-04 9.77549594e-05 8.55786413e-05 7.95106643e-05\n",
      " 7.88753124e-05 7.21744161e-05 6.99835068e-05 6.72955467e-05\n",
      " 6.13183234e-05 5.77830123e-05 5.34717696e-05 4.95535934e-05\n",
      " 4.89297157e-05 4.63514398e-05 4.51214744e-05 4.14095643e-05\n",
      " 3.90245458e-05 3.22314797e-05 3.07010213e-05 2.76989236e-05\n",
      " 2.22711326e-05 2.12295058e-05 1.97317790e-05 1.66301243e-05\n",
      " 1.65307256e-05 1.41800778e-05 1.29642066e-05 1.20955677e-05\n",
      " 9.87999729e-06 8.99758663e-06 8.49630432e-06 8.10539891e-06\n",
      " 5.78711653e-06 5.24569752e-06 5.05376749e-06 4.04380755e-06\n",
      " 4.01141465e-06 3.62649933e-06 3.16163999e-06 2.53411720e-06\n",
      " 1.99401296e-06 1.20576219e-06 1.02694608e-06 3.90362783e-07\n",
      " 1.37308242e-32 1.74372756e-33 1.45703810e-33 1.29222709e-33\n",
      " 1.02904817e-33 1.02741369e-33 8.45803750e-34 7.63794819e-34\n",
      " 4.97677871e-34 4.29917031e-34 3.21672512e-34 3.21672512e-34\n",
      " 3.21672512e-34 3.21672512e-34 3.21672512e-34 3.21672512e-34\n",
      " 3.21672512e-34 3.21672512e-34 3.21672512e-34 3.21672512e-34\n",
      " 3.21672512e-34 3.21672512e-34 3.21672512e-34 3.21672512e-34\n",
      " 3.21672512e-34 3.21672512e-34 3.21672512e-34 3.21672512e-34\n",
      " 3.21672512e-34 3.21672512e-34 3.21672512e-34 3.21672512e-34\n",
      " 3.21672512e-34 3.21672512e-34 3.21672512e-34 3.21672512e-34\n",
      " 3.21672512e-34 3.21672512e-34 3.21672512e-34 3.21672512e-34\n",
      " 3.21672512e-34 3.21672512e-34 3.21672512e-34 3.21672512e-34\n",
      " 3.21672512e-34 3.21672512e-34 3.21672512e-34 3.21672512e-34\n",
      " 3.21672512e-34 3.21672512e-34 3.21672512e-34 3.21672512e-34\n",
      " 3.21672512e-34 3.21672512e-34 3.21672512e-34 3.21672512e-34\n",
      " 3.21672512e-34 3.21672512e-34 3.21672512e-34 3.21672512e-34\n",
      " 3.21672512e-34 3.21672512e-34 3.21672512e-34 3.21672512e-34\n",
      " 3.21672512e-34 3.21672512e-34 3.21672512e-34 3.21672512e-34\n",
      " 3.21672512e-34 3.21672512e-34 3.21672512e-34 3.21672512e-34\n",
      " 3.21672512e-34 3.21672512e-34 3.21672512e-34 3.21672512e-34\n",
      " 3.21672512e-34 3.21672512e-34 3.21672512e-34 2.65775202e-34\n",
      " 1.22337132e-34 1.06145954e-34 1.05677628e-34 1.05886637e-35] \n",
      "\n",
      "\n",
      "\n",
      "Graph for variance ratio : \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None \n",
      "\n",
      "\n",
      "\n",
      "Explained variance ratio of Scaled Dataset with cumulative sum : \n",
      "\n",
      "[0.06962355 0.12578082 0.17216166 0.20647386 0.23887799 0.27032457\n",
      " 0.29858364 0.31974453 0.33895376 0.35626301 0.37259183 0.38854936\n",
      " 0.40316268 0.41704853 0.43029095 0.4426781  0.45501997 0.46655846\n",
      " 0.47783153 0.48869804 0.49867696 0.50800192 0.51731493 0.52636354\n",
      " 0.5350037  0.54312731 0.55051272 0.55788319 0.56518334 0.57215161\n",
      " 0.57884724 0.5854697  0.59190052 0.59816186 0.60431356 0.61038243\n",
      " 0.61626847 0.6220302  0.62762915 0.63316031 0.63854478 0.64386881\n",
      " 0.64903698 0.65416731 0.6592039  0.6641583  0.66904619 0.67384288\n",
      " 0.67843637 0.68300701 0.68741253 0.69173769 0.6960177  0.70021071\n",
      " 0.70439255 0.70846129 0.71245391 0.71636152 0.72019521 0.72397013\n",
      " 0.7276844  0.73132889 0.7349593  0.73853394 0.74209851 0.7456296\n",
      " 0.74910451 0.75249435 0.7558253  0.75914676 0.76243247 0.76566239\n",
      " 0.76886191 0.77203367 0.77517956 0.77828561 0.78136062 0.78441271\n",
      " 0.7874512  0.79045634 0.79345037 0.79641129 0.7993652  0.80229497\n",
      " 0.80521141 0.80810519 0.81098574 0.81385336 0.81668255 0.81946445\n",
      " 0.82224009 0.82499555 0.82774568 0.83047847 0.83316822 0.83585046\n",
      " 0.83851896 0.84116643 0.84380257 0.84639858 0.84898688 0.85154401\n",
      " 0.85409343 0.85660319 0.85907976 0.86154208 0.8639995  0.86643439\n",
      " 0.86885887 0.87125639 0.87363598 0.87600375 0.87835454 0.88066623\n",
      " 0.8829596  0.88522425 0.88746446 0.88968053 0.89188512 0.89406924\n",
      " 0.89623196 0.89834599 0.90043317 0.9024968  0.90455816 0.90660435\n",
      " 0.90863028 0.9106281  0.91260529 0.91456747 0.91650742 0.91842813\n",
      " 0.92030569 0.92216617 0.92398068 0.92578199 0.92754593 0.92930246\n",
      " 0.93103652 0.93276397 0.93442271 0.9360762  0.93771116 0.93930395\n",
      " 0.94086896 0.94241313 0.94393333 0.94536438 0.94677139 0.94816839\n",
      " 0.94951644 0.9508404  0.95212599 0.95339213 0.9546211  0.95582566\n",
      " 0.9570291  0.9582121  0.95936802 0.96050429 0.96162894 0.96272116\n",
      " 0.96379099 0.96482404 0.96582612 0.96680653 0.96777609 0.96870332\n",
      " 0.96961625 0.97052269 0.97140932 0.9722829  0.9731406  0.9739605\n",
      " 0.97476926 0.97556183 0.97633534 0.97709875 0.97783797 0.9785511\n",
      " 0.97923871 0.9799166  0.98057338 0.98119838 0.98181311 0.98240167\n",
      " 0.98298209 0.98355179 0.98410531 0.9846425  0.98517094 0.98568715\n",
      " 0.98618911 0.98667432 0.98715621 0.98763471 0.98810012 0.98855058\n",
      " 0.98899152 0.98941921 0.98982852 0.99023168 0.99062796 0.99102002\n",
      " 0.99140858 0.99177757 0.99213615 0.99248219 0.992824   0.99316134\n",
      " 0.99348728 0.99380215 0.99410238 0.99438182 0.99465733 0.9949268\n",
      " 0.99518674 0.99544309 0.99569596 0.99594044 0.9961731  0.99638659\n",
      " 0.99659058 0.99678816 0.99697453 0.99715258 0.99732563 0.99749483\n",
      " 0.99765975 0.99782054 0.99796593 0.99810753 0.99824207 0.99836814\n",
      " 0.99848735 0.99860036 0.99870604 0.99880379 0.99888937 0.99896888\n",
      " 0.99904776 0.99911993 0.99918992 0.99925721 0.99931853 0.99937631\n",
      " 0.99942978 0.99947934 0.99952827 0.99957462 0.99961974 0.99966115\n",
      " 0.99970018 0.99973241 0.99976311 0.99979081 0.99981308 0.99983431\n",
      " 0.99985404 0.99987067 0.9998872  0.99990138 0.99991434 0.99992644\n",
      " 0.99993632 0.99994532 0.99995381 0.99996192 0.99996771 0.99997295\n",
      " 0.99997801 0.99998205 0.99998606 0.99998969 0.99999285 0.99999538\n",
      " 0.99999738 0.99999858 0.99999961 1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.        ] \n",
      "\n",
      "\n",
      "\n",
      "Graph for Cumulative sum of varaince ratio : \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAANRUlEQVR4nO3dX4yc112H8edbu5YCVA3CC1T+gy1k0vqiEWVJuKkaqAA7SFhIRUqKWhE1siLFiMv4Ci56E1SKoCKttYpMqJDwDRFYjWlASNBWJZI3UpXWrdKuHIgXB8Vpq1BSwHLy42LHyXgynnnXO7s7c+b5SKvd97xnZ44i5dmj45ndVBWSpNn3ju1egCRpMgy6JDXCoEtSIwy6JDXCoEtSI3Zu1xPv3r27Dhw4sF1PL0kz6dlnn32lqhaG3du2oB84cIDl5eXtenpJmklJ/v1m9zxykaRGGHRJaoRBl6RGGHRJaoRBl6RGGHRJasTYoCc5neTlJN+4yf0k+UySlSTPJfnA5JcpSRqnyw79CeDIiPtHgUO9j+PA5za+LEnSeo0NelV9CfjeiCnHgM/XmmeA25O8Z1ILlCR1M4l3iu4BLvVdr/bGXhqcmOQ4a7t49u/fP4Gnltp04ORTb379b4/+xg3Xo8anaWza1nOzMWDLn/v6807aJIKeIWND/wxSVS0BSwCLi4v+qSQ17/r/xBsNo9TFJIK+Cuzru94LXJ7A40pT7cDJp4yypsokgn4WOJHkDHA38GpVve24RZo1hlqzZmzQk/w1cA+wO8kq8IfAOwGq6hRwDrgXWAF+CDywWYuVNsvNzlalWTI26FV1/5j7BTw8sRVJm8hwq2Xb9vvQpa2wFa8skKaFb/1XUw6cfOrNiHverXnjDl1NMN6SQdcM8hxcGs4jF82E/qMUScO5Q9dUM+JSd+7QNXXcjUu3xh26poYRlzbGHbq2lbtxaXLcoWtbGHFp8tyha0sZcmnzuEPXpjPi0tZwhy5JjXCHronzF2JJ28Oga2I8WpG2l0cuktQIg64N8XXk0vTwyEW3xIhL08cdutbFkEvTyx26OjHk0vRzh66xjLk0Gwy6bsqQS7PFIxe9jSGXZpM7dN3AmEuzy6ALMORSCwy6jLnUCM/Q55ghl9riDl2SGmHQ54y/e0Vql0cuc8KIS+1zhy5JjTDojXNnLs2PTkcuSY4AfwbsAB6vqkcH7r8b+Ctgf+8x/7iq/mLCa9U6GHJp/ozdoSfZATwGHAUOA/cnOTww7WHgm1V1J3AP8Okkuya8VnVkzKX51OXI5S5gpaouVtVV4AxwbGBOAe9KEuDHgO8B1ya6UknSSF2Cvge41He92hvr9+fA+4DLwNeB36+qNwYfKMnxJMtJlq9cuXKLS9YwvhxRUpcz9AwZq4HrXwe+BvwK8LPAPyb5clX91w3fVLUELAEsLi4OPoZugRGXdF2XHfoqsK/vei9rO/F+DwBP1poV4AXgvZNZoiSpiy5BPw8cSnKw9w+d9wFnB+a8CHwYIMlPAXcAFye5UN3InbmkQWOPXKrqWpITwNOsvWzxdFVdSPJQ7/4p4JPAE0m+ztoRzSNV9comrnuuGXNJw3R6HXpVnQPODYyd6vv6MvBrk12aBhlySaP4TtEZYcwljWPQJakRBn3KuTOX1JW/PndKGXJJ6+UOfQoZc0m3wqBLUiMM+hRxZy5pIwz6lDDmkjbKfxTdZoZc0qS4Q5ekRhh0SWqEQd8G/jEKSZvBoG8xQy5psxj0LWTMJW0mgy5JjTDoktQIg74FPGqRtBV8Y9EmMuSStpI79E1izCVtNYMuSY0w6JLUCIM+YR61SNouBn2CjLmk7WTQJ8SYS9puBn0CjLmkaWDQJakRBl2SGmHQJakRBn0DPDuXNE0M+i0y5pKmjUG/BcZc0jQy6OtkzCVNq05BT3IkyfNJVpKcvMmce5J8LcmFJP8y2WVKksYZ+/vQk+wAHgN+FVgFzic5W1Xf7JtzO/BZ4EhVvZjkJzdpvZKkm+iyQ78LWKmqi1V1FTgDHBuY81Hgyap6EaCqXp7sMiVJ43QJ+h7gUt/1am+s388BP57kn5M8m+Tjk1rgtPDsXNK06/In6DJkrIY8zi8AHwZuA/41yTNV9e0bHig5DhwH2L9///pXu02MuaRZ0GWHvgrs67veC1weMueLVfVaVb0CfAm4c/CBqmqpqharanFhYeFW17yljLmkWdEl6OeBQ0kOJtkF3AecHZjzd8AHk+xM8iPA3cC3JrtUSdIoY49cqupakhPA08AO4HRVXUjyUO/+qar6VpIvAs8BbwCPV9U3NnPhW8HduaRZ0uUMnao6B5wbGDs1cP0p4FOTW5okaT18p6gkNcKgS1IjDPoQnp1LmkUGfYAxlzSrDLokNcKg93F3LmmWGfQeYy5p1hl0SWqEQcfduaQ2GHRJaoRBl6RGGHRJaoRBl6RGGHRJasRcB91Xt0hqyVwHXZJaMrdBd3cuqTVzG3RJas1cBt3duaQWzWXQJalFcxd0d+eSWjV3QZekVhl0SWqEQZekRhh0SWqEQZekRhh0SWrE3ATdlytKat3cBF2SWmfQJakRcxF0j1skzYO5CLokzYPmg+7uXNK86BT0JEeSPJ9kJcnJEfN+McnrST4yuSVKkroYG/QkO4DHgKPAYeD+JIdvMu+PgKcnvUhJ0nhdduh3AStVdbGqrgJngGND5v0e8DfAyxNc34Z43CJpnnQJ+h7gUt/1am/sTUn2AL8FnBr1QEmOJ1lOsnzlypX1rlWSNEKXoGfIWA1c/ynwSFW9PuqBqmqpqharanFhYaHjEiVJXezsMGcV2Nd3vRe4PDBnETiTBGA3cG+Sa1X1t5NYpCRpvC5BPw8cSnIQ+A/gPuCj/ROq6uD1r5M8AXzBmEvS1hob9Kq6luQEa69e2QGcrqoLSR7q3R95bi5J2hpdduhU1Tng3MDY0JBX1e9ufFmSpPVq/p2ikjQvDLokNcKgS1Ijmgy67xCVNI+aDLokzSODLkmNMOiS1Ijmgu75uaR51VzQJWleGXRJaoRBl6RGNBV0z88lzbOmgi5J88ygS1IjDLokNaKZoHt+LmneNRN0SZp3Bl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGtEp6EmOJHk+yUqSk0Pu/06S53ofX01y5+SXKkkaZWzQk+wAHgOOAoeB+5McHpj2AvChqno/8ElgadILlSSN1mWHfhewUlUXq+oqcAY41j+hqr5aVd/vXT4D7J3sMm/OPz0nSWu6BH0PcKnverU3djOfAP5+2I0kx5MsJ1m+cuVK91VKksbqEvQMGauhE5NfZi3ojwy7X1VLVbVYVYsLCwvdVylJGmtnhzmrwL6+673A5cFJSd4PPA4crarvTmZ5kqSuuuzQzwOHkhxMsgu4DzjbPyHJfuBJ4GNV9e3JL1OSNM7YHXpVXUtyAnga2AGcrqoLSR7q3T8F/AHwE8BnkwBcq6rFzVu2JGlQlyMXquoccG5g7FTf1w8CD052aZKk9Zjpd4r6kkVJestMB12S9BaDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1IiZDbrvEpWkG81s0CVJNzLoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktSImQy6b/uXpLebyaBLkt7OoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDWiU9CTHEnyfJKVJCeH3E+Sz/TuP5fkA5NfqiRplLFBT7IDeAw4ChwG7k9yeGDaUeBQ7+M48LkJr1OSNEaXHfpdwEpVXayqq8AZ4NjAnGPA52vNM8DtSd4z4bVKkkZIVY2ekHwEOFJVD/auPwbcXVUn+uZ8AXi0qr7Su/4n4JGqWh54rOOs7eAB7gCe38DadwPvAn4wML6RsY1+/3aNTdt6XON8r3FW172Vz/3CkOfu6meqamHYjZ0dvjlDxgZ/CnSZQ1UtAUsdnnP8opJlYIG3/4fZyNhGv3+7xqZtPa5xvtc4q+vesueuqsUhz71hXY5cVoF9fdd7gcu3MEeStIm6BP08cCjJwSS7gPuAswNzzgIf773a5ZeAV6vqpQmvVZI0wtgjl6q6luQE8DSwAzhdVReSPNS7fwo4B9wLrAA/BB7YvCW/aQn4IPDlgfGNjG30+7drbNrW4xrne42zuu6tfO5NMfYfRSVJs8F3ikpSIwy6JDWiy8sWt1yS1/GHjaT5Vqy9JPx14H+Bh6vqL0d9w1SeoSf5777LHwXewMBLakN/qK9vXnf2xi8B/wm8tzf2A+D/gEeBP6mq20Y9sJGUpK2Vvs87WAv3G6zF/TZgD2uvHHwJ2MXapvb7dDhRcYcuSdPnKnAS+DRv7eT/oap+c9Q3TWvQPUOX1JLrxyw3cxV4J2vh3glc6429yloL3wC+U1UfGvUk0xrN/wFe633Q93nQ9P00kqTRirVA93sHa8G/fqzyDHAReDdrvw/mt4H9SXaPeuCpfJXLOoz6iSdJ0yi81a7rm9JXgJ/uu/753uddwBXWNrm7gO+OfOApPXIZPEN/rfdZklp3PcoZGPtOVd0x6hunMuiSpPWb1jN0SdI6GXRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RG/D+Jix5b2HT+7QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None \n",
      "\n",
      "\n",
      "\n",
      "Shape of Dataset after dimensionality reduction : \n",
      "\n",
      "(8418, 123) \n",
      "\n",
      "\n",
      "\n",
      "Dataset after dimensionality reduction : \n",
      "\n",
      "            PC0        PC1       PC2       PC3       PC4       PC5       PC6  \\\n",
      "0     11.580577  -1.908280 -1.645008  1.290739 -0.907196 -2.328201  7.854727   \n",
      "1     -0.133096   0.353761  0.829697  1.140162 -2.406473  0.347280  1.021301   \n",
      "2     10.451144  21.494189 -6.071369 -3.742154  1.394463  2.533316  0.762914   \n",
      "3      7.349810  21.238342 -6.796905  0.736425  2.141557 -0.143797  0.289508   \n",
      "4      6.629969  21.411149 -7.356675  1.441374  2.137933 -0.885920 -1.016445   \n",
      "...         ...        ...       ...       ...       ...       ...       ...   \n",
      "8413  -1.526370   1.537673  5.776801 -0.220262 -0.976992 -1.134596 -3.433141   \n",
      "8414   0.088926  -2.042986 -4.042179  0.574480 -2.544394  1.791152  3.068706   \n",
      "8415  -2.664512   0.808127  3.117285  2.890138 -0.462366 -1.568877 -3.866196   \n",
      "8416  -1.254102   0.469456  4.182495 -4.459709  0.060282 -0.463933  1.427608   \n",
      "8417  -1.463493  -2.093108 -3.445282  2.440254 -4.823620  2.691580  0.981778   \n",
      "\n",
      "           PC7        PC8       PC9  ...     PC113     PC114     PC115  \\\n",
      "0    -1.950396 -13.069384 -1.048823  ... -0.560770 -0.658221 -0.247455   \n",
      "1    -0.771919  -0.383078  0.014709  ...  0.061214 -0.117142 -0.776560   \n",
      "2     3.523860  -0.987806 -0.819300  ... -2.714473  1.614915 -1.376188   \n",
      "3    -0.537676   0.503757 -3.957188  ...  1.383613 -0.323443 -1.108093   \n",
      "4    -0.879974   1.318950 -1.672020  ... -1.271267  1.270644 -1.278067   \n",
      "...        ...        ...       ...  ...       ...       ...       ...   \n",
      "8413  0.499018   0.501311  2.096024  ...  0.339813  0.051889  0.236492   \n",
      "8414  3.343465  -2.472185  1.287871  ... -0.765019  0.575813  0.594071   \n",
      "8415  1.073500  -0.146038  0.831529  ...  1.262324 -1.478074  0.315519   \n",
      "8416  0.607461   0.822807  0.757675  ... -0.432381  0.289996 -1.476170   \n",
      "8417  4.278793   0.003636 -1.625849  ...  0.247764 -0.185627  0.273550   \n",
      "\n",
      "         PC116     PC117     PC118     PC119     PC120     PC121     PC122  \n",
      "0     0.513131 -0.592175  1.217348 -0.341925 -0.779232 -0.368804  0.433686  \n",
      "1     1.004506 -0.995470  1.431915 -0.118116 -0.770408  0.003983 -2.348176  \n",
      "2    -3.176163 -5.381098  1.845317  6.552925 -6.922002 -1.694684 -7.779917  \n",
      "3    -0.079159 -0.464203  0.910741 -0.521385  0.438168 -0.683763  0.700810  \n",
      "4    -3.053061 -5.103573  1.769064  4.230389 -2.951015 -1.260393 -6.785916  \n",
      "...        ...       ...       ...       ...       ...       ...       ...  \n",
      "8413  0.601017  0.517634  0.416976  0.112931  0.512860  0.147691 -1.712074  \n",
      "8414 -0.238308 -0.324755 -1.034982  0.431992 -1.788777 -1.551038 -0.872496  \n",
      "8415 -0.555632  1.356840 -0.838052  0.598379 -0.156964 -0.526858 -0.690382  \n",
      "8416 -0.045491  0.434734 -0.971413 -0.019800  0.193165  0.290545  0.428145  \n",
      "8417 -0.063707  0.173325 -0.207918  0.653677 -0.046337 -0.128869 -0.247337  \n",
      "\n",
      "[8418 rows x 123 columns] \n",
      "\n",
      "\n",
      "\n",
      "Train acc :  0.9558843695157845 \n",
      "\n",
      "Predicted target variable using test_df :  [ 73.84491  90.63363  81.8471  ...  98.09098 109.03973  92.53824]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#Loading Datasets\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print('Training Info : \\n')\n",
    "print(train_df.info(), '\\n\\n\\n')\n",
    "\n",
    "print('Training Data : \\n')\n",
    "print(train_df.head(),'\\n\\n\\n')\n",
    "\n",
    "print('Test Data : \\n')\n",
    "print(test_df.head(), '\\n\\n\\n')\n",
    "\n",
    "print('Train Dataframe shape : \\n')\n",
    "print(train_df.shape,'\\n\\n\\n')\n",
    "print('Test Dataframe shape : \\n')\n",
    "print(test_df.shape,'\\n\\n\\n')\n",
    "\n",
    "train_y = train_df['y']\n",
    "train_df = train_df.drop(['ID', 'y'], axis = 1)\n",
    "test_df = test_df.drop('ID', axis = 1)\n",
    "\n",
    "print('Train Dataframe after dropping ID and y columns\\n')\n",
    "print(train_df.head(), '\\n\\n\\n')\n",
    "print('Test dataframe after dropping ID column\\n')\n",
    "print(test_df.head(), '\\n\\n\\n')\n",
    "\n",
    "## Removing variable having variance 0\n",
    "\n",
    "for i in train_df.columns:\n",
    "    if train_df[i].dtype != object:\n",
    "        if train_df[i].var() == 0:\n",
    "            train_df = train_df.drop(i, axis = 1)\n",
    "            test_df = test_df.drop(i, axis = 1) \n",
    "total_df = pd.concat([train_df, test_df])\n",
    "\n",
    "\n",
    "print('Train Dataframe after removing columns with variance 0 : \\n')\n",
    "print(train_df.head(), '\\n\\n\\n')\n",
    "\n",
    "print('Test Dataframe after removing columns with variance 0 : \\n')\n",
    "print(test_df.head(), '\\n\\n\\n')\n",
    "\n",
    "print('Dataframe after concatinating train_df and test_df\\n')\n",
    "print(total_df, '\\n\\n\\n')\n",
    "\n",
    "\n",
    "## Check for null and unique values for test and train sets.\n",
    "print('Check null values for train sets. \\n')\n",
    "print(train_df.columns[train_df.isnull().any()])\n",
    "print(train_df[train_df.isnull().any(axis = 1)], '\\n\\n\\n')\n",
    "\n",
    "print('Check null values for test sets.\\n')\n",
    "print(test_df.columns[test_df.isnull().any()])\n",
    "print(test_df[test_df.isnull().any(axis = 1)],'\\n\\n\\n')\n",
    "\n",
    "## Apply label encoder.\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "for i in total_df.columns:\n",
    "    if total_df[i].dtype == object:\n",
    "        total_df[i] = le.fit_transform(total_df[i])\n",
    "\n",
    "print('Total Dataframe after applying Label Encoder : \\n')\n",
    "print(total_df, '\\n\\n\\n')\n",
    "\n",
    "train_df = total_df[:len(train_df)]\n",
    "test_df = total_df[len(train_df):]\n",
    "\n",
    "print('Train Dataframe after applying Label Encoder\\n')\n",
    "print(train_df, '\\n\\n\\n')\n",
    "\n",
    "print('Test Dataframe after applying Label Encoder\\n')\n",
    "print(test_df, '\\n\\n\\n')\n",
    "\n",
    "## Dimensonality reduction\n",
    "\n",
    "## Using PCA for dimensionality reduction \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(total_df)\n",
    "\n",
    "print('Scaled Dataset with Standard Scaler : \\n')\n",
    "print(X_scaled.shape, '\\n\\n\\n')\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "print('Explained variance ratio of Scaled Dataset \\n')\n",
    "print(pca.explained_variance_ratio_, '\\n\\n\\n')\n",
    "print('Graph for variance ratio : \\n')\n",
    "plt.plot(['PC'+ str(i) for i in range(364)],pca.explained_variance_ratio_)\n",
    "print(plt.show(), '\\n\\n\\n')\n",
    "print('Explained variance ratio of Scaled Dataset with cumulative sum : \\n')\n",
    "print(pca.explained_variance_ratio_.cumsum(), '\\n\\n\\n')\n",
    "\n",
    "print('Graph for Cumulative sum of varaince ratio : \\n')\n",
    "plt.bar(['PC'+ str(i) for i in range(364)],pca.explained_variance_ratio_.cumsum())\n",
    "print(plt.show(), '\\n\\n\\n')\n",
    "pca = PCA(n_components = 0.90)\n",
    "\n",
    "PCA_X = pca.fit_transform(X_scaled)\n",
    "\n",
    "print('Shape of Dataset after dimensionality reduction : \\n')\n",
    "print(PCA_X.shape, '\\n\\n\\n')\n",
    "\n",
    "PCA_df = pd.DataFrame(PCA_X, columns = ['PC' + str(i) for i in range(123)])\n",
    "print('Dataset after dimensionality reduction : \\n')\n",
    "print(PCA_df, '\\n\\n\\n')\n",
    "train_df = PCA_X[:len(train_df)]\n",
    "test_df = PCA_X[len(train_df):]\n",
    "\n",
    "\n",
    "##  Applying XGBoost algorithm \n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "xgb = XGBRegressor()\n",
    "xgb.fit(train_df, train_y)\n",
    "print('Train acc : ', xgb.score(train_df, train_y), '\\n')\n",
    "predected_y = xgb.predict(test_df)\n",
    "print('Predicted target variable using test_df : ',predected_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e375d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Info : \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4209 entries, 0 to 4208\n",
      "Columns: 378 entries, ID to X385\n",
      "dtypes: float64(1), int64(369), object(8)\n",
      "memory usage: 12.1+ MB\n",
      "None \n",
      "\n",
      "\n",
      "\n",
      "Training Data : \n",
      "\n",
      "   ID       y  X0 X1  X2 X3 X4 X5 X6 X8  ...  X375  X376  X377  X378  X379  \\\n",
      "0   0  130.81   k  v  at  a  d  u  j  o  ...     0     0     1     0     0   \n",
      "1   6   88.53   k  t  av  e  d  y  l  o  ...     1     0     0     0     0   \n",
      "2   7   76.26  az  w   n  c  d  x  j  x  ...     0     0     0     0     0   \n",
      "3   9   80.62  az  t   n  f  d  x  l  e  ...     0     0     0     0     0   \n",
      "4  13   78.02  az  v   n  f  d  h  d  n  ...     0     0     0     0     0   \n",
      "\n",
      "   X380  X382  X383  X384  X385  \n",
      "0     0     0     0     0     0  \n",
      "1     0     0     0     0     0  \n",
      "2     0     1     0     0     0  \n",
      "3     0     0     0     0     0  \n",
      "4     0     0     0     0     0  \n",
      "\n",
      "[5 rows x 378 columns] \n",
      "\n",
      "\n",
      "\n",
      "Test Data : \n",
      "\n",
      "   ID  X0 X1  X2 X3 X4 X5 X6 X8  X10  ...  X375  X376  X377  X378  X379  X380  \\\n",
      "0   1  az  v   n  f  d  t  a  w    0  ...     0     0     0     1     0     0   \n",
      "1   2   t  b  ai  a  d  b  g  y    0  ...     0     0     1     0     0     0   \n",
      "2   3  az  v  as  f  d  a  j  j    0  ...     0     0     0     1     0     0   \n",
      "3   4  az  l   n  f  d  z  l  n    0  ...     0     0     0     1     0     0   \n",
      "4   5   w  s  as  c  d  y  i  m    0  ...     1     0     0     0     0     0   \n",
      "\n",
      "   X382  X383  X384  X385  \n",
      "0     0     0     0     0  \n",
      "1     0     0     0     0  \n",
      "2     0     0     0     0  \n",
      "3     0     0     0     0  \n",
      "4     0     0     0     0  \n",
      "\n",
      "[5 rows x 377 columns] \n",
      "\n",
      "\n",
      "\n",
      "Train Dataframe shape : \n",
      "\n",
      "(4209, 378) \n",
      "\n",
      "\n",
      "\n",
      "Test Dataframe shape : \n",
      "\n",
      "(4209, 377) \n",
      "\n",
      "\n",
      "\n",
      "Train Dataframe after dropping ID and y columns\n",
      "\n",
      "   X0 X1  X2 X3 X4 X5 X6 X8  X10  X11  ...  X375  X376  X377  X378  X379  \\\n",
      "0   k  v  at  a  d  u  j  o    0    0  ...     0     0     1     0     0   \n",
      "1   k  t  av  e  d  y  l  o    0    0  ...     1     0     0     0     0   \n",
      "2  az  w   n  c  d  x  j  x    0    0  ...     0     0     0     0     0   \n",
      "3  az  t   n  f  d  x  l  e    0    0  ...     0     0     0     0     0   \n",
      "4  az  v   n  f  d  h  d  n    0    0  ...     0     0     0     0     0   \n",
      "\n",
      "   X380  X382  X383  X384  X385  \n",
      "0     0     0     0     0     0  \n",
      "1     0     0     0     0     0  \n",
      "2     0     1     0     0     0  \n",
      "3     0     0     0     0     0  \n",
      "4     0     0     0     0     0  \n",
      "\n",
      "[5 rows x 376 columns] \n",
      "\n",
      "\n",
      "\n",
      "Test dataframe after dropping ID column\n",
      "\n",
      "   X0 X1  X2 X3 X4 X5 X6 X8  X10  X11  ...  X375  X376  X377  X378  X379  \\\n",
      "0  az  v   n  f  d  t  a  w    0    0  ...     0     0     0     1     0   \n",
      "1   t  b  ai  a  d  b  g  y    0    0  ...     0     0     1     0     0   \n",
      "2  az  v  as  f  d  a  j  j    0    0  ...     0     0     0     1     0   \n",
      "3  az  l   n  f  d  z  l  n    0    0  ...     0     0     0     1     0   \n",
      "4   w  s  as  c  d  y  i  m    0    0  ...     1     0     0     0     0   \n",
      "\n",
      "   X380  X382  X383  X384  X385  \n",
      "0     0     0     0     0     0  \n",
      "1     0     0     0     0     0  \n",
      "2     0     0     0     0     0  \n",
      "3     0     0     0     0     0  \n",
      "4     0     0     0     0     0  \n",
      "\n",
      "[5 rows x 376 columns] \n",
      "\n",
      "\n",
      "\n",
      "Train Dataframe after removing columns with varaince 0 : \n",
      "\n",
      "   X0 X1  X2 X3 X4 X5 X6 X8  X10  X12  ...  X375  X376  X377  X378  X379  \\\n",
      "0   k  v  at  a  d  u  j  o    0    0  ...     0     0     1     0     0   \n",
      "1   k  t  av  e  d  y  l  o    0    0  ...     1     0     0     0     0   \n",
      "2  az  w   n  c  d  x  j  x    0    0  ...     0     0     0     0     0   \n",
      "3  az  t   n  f  d  x  l  e    0    0  ...     0     0     0     0     0   \n",
      "4  az  v   n  f  d  h  d  n    0    0  ...     0     0     0     0     0   \n",
      "\n",
      "   X380  X382  X383  X384  X385  \n",
      "0     0     0     0     0     0  \n",
      "1     0     0     0     0     0  \n",
      "2     0     1     0     0     0  \n",
      "3     0     0     0     0     0  \n",
      "4     0     0     0     0     0  \n",
      "\n",
      "[5 rows x 364 columns] \n",
      "\n",
      "\n",
      "\n",
      "Test Dataframe after removing columns with varaince 0 : \n",
      "\n",
      "   X0 X1  X2 X3 X4 X5 X6 X8  X10  X12  ...  X375  X376  X377  X378  X379  \\\n",
      "0  az  v   n  f  d  t  a  w    0    0  ...     0     0     0     1     0   \n",
      "1   t  b  ai  a  d  b  g  y    0    0  ...     0     0     1     0     0   \n",
      "2  az  v  as  f  d  a  j  j    0    0  ...     0     0     0     1     0   \n",
      "3  az  l   n  f  d  z  l  n    0    0  ...     0     0     0     1     0   \n",
      "4   w  s  as  c  d  y  i  m    0    0  ...     1     0     0     0     0   \n",
      "\n",
      "   X380  X382  X383  X384  X385  \n",
      "0     0     0     0     0     0  \n",
      "1     0     0     0     0     0  \n",
      "2     0     0     0     0     0  \n",
      "3     0     0     0     0     0  \n",
      "4     0     0     0     0     0  \n",
      "\n",
      "[5 rows x 364 columns] \n",
      "\n",
      "\n",
      "\n",
      "Dataframe after concatinating train_df and test_df\n",
      "\n",
      "      X0  X1  X2 X3 X4  X5 X6 X8  X10  X12  ...  X375  X376  X377  X378  X379  \\\n",
      "0      k   v  at  a  d   u  j  o    0    0  ...     0     0     1     0     0   \n",
      "1      k   t  av  e  d   y  l  o    0    0  ...     1     0     0     0     0   \n",
      "2     az   w   n  c  d   x  j  x    0    0  ...     0     0     0     0     0   \n",
      "3     az   t   n  f  d   x  l  e    0    0  ...     0     0     0     0     0   \n",
      "4     az   v   n  f  d   h  d  n    0    0  ...     0     0     0     0     0   \n",
      "...   ..  ..  .. .. ..  .. .. ..  ...  ...  ...   ...   ...   ...   ...   ...   \n",
      "4204  aj   h  as  f  d  aa  j  e    0    0  ...     0     0     0     0     0   \n",
      "4205   t  aa  ai  d  d  aa  j  y    0    0  ...     0     1     0     0     0   \n",
      "4206   y   v  as  f  d  aa  d  w    0    0  ...     0     0     0     0     0   \n",
      "4207  ak   v  as  a  d  aa  c  q    0    0  ...     0     0     1     0     0   \n",
      "4208   t  aa  ai  c  d  aa  g  r    0    0  ...     1     0     0     0     0   \n",
      "\n",
      "      X380  X382  X383  X384  X385  \n",
      "0        0     0     0     0     0  \n",
      "1        0     0     0     0     0  \n",
      "2        0     1     0     0     0  \n",
      "3        0     0     0     0     0  \n",
      "4        0     0     0     0     0  \n",
      "...    ...   ...   ...   ...   ...  \n",
      "4204     0     0     0     0     0  \n",
      "4205     0     0     0     0     0  \n",
      "4206     0     0     0     0     0  \n",
      "4207     0     0     0     0     0  \n",
      "4208     0     0     0     0     0  \n",
      "\n",
      "[8418 rows x 364 columns] \n",
      "\n",
      "\n",
      "\n",
      "Check null values for train sets. \n",
      "\n",
      "Index([], dtype='object')\n",
      "Empty DataFrame\n",
      "Columns: [X0, X1, X2, X3, X4, X5, X6, X8, X10, X12, X13, X14, X15, X16, X17, X18, X19, X20, X21, X22, X23, X24, X26, X27, X28, X29, X30, X31, X32, X33, X34, X35, X36, X37, X38, X39, X40, X41, X42, X43, X44, X45, X46, X47, X48, X49, X50, X51, X52, X53, X54, X55, X56, X57, X58, X59, X60, X61, X62, X63, X64, X65, X66, X67, X68, X69, X70, X71, X73, X74, X75, X76, X77, X78, X79, X80, X81, X82, X83, X84, X85, X86, X87, X88, X89, X90, X91, X92, X94, X95, X96, X97, X98, X99, X100, X101, X102, X103, X104, X105, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 364 columns] \n",
      "\n",
      "\n",
      "\n",
      "Check null values for test sets.\n",
      "\n",
      "Index([], dtype='object')\n",
      "Empty DataFrame\n",
      "Columns: [X0, X1, X2, X3, X4, X5, X6, X8, X10, X12, X13, X14, X15, X16, X17, X18, X19, X20, X21, X22, X23, X24, X26, X27, X28, X29, X30, X31, X32, X33, X34, X35, X36, X37, X38, X39, X40, X41, X42, X43, X44, X45, X46, X47, X48, X49, X50, X51, X52, X53, X54, X55, X56, X57, X58, X59, X60, X61, X62, X63, X64, X65, X66, X67, X68, X69, X70, X71, X73, X74, X75, X76, X77, X78, X79, X80, X81, X82, X83, X84, X85, X86, X87, X88, X89, X90, X91, X92, X94, X95, X96, X97, X98, X99, X100, X101, X102, X103, X104, X105, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 364 columns] \n",
      "\n",
      "\n",
      "\n",
      "Total Dataframe after applying Label Encoder : \n",
      "\n",
      "      X0  X1  X2  X3  X4  X5  X6  X8  X10  X12  ...  X375  X376  X377  X378  \\\n",
      "0     37  23  20   0   3  27   9  14    0    0  ...     0     0     1     0   \n",
      "1     37  21  22   4   3  31  11  14    0    0  ...     1     0     0     0   \n",
      "2     24  24  38   2   3  30   9  23    0    0  ...     0     0     0     0   \n",
      "3     24  21  38   5   3  30  11   4    0    0  ...     0     0     0     0   \n",
      "4     24  23  38   5   3  14   3  13    0    0  ...     0     0     0     0   \n",
      "...   ..  ..  ..  ..  ..  ..  ..  ..  ...  ...  ...   ...   ...   ...   ...   \n",
      "4204   9   9  19   5   3   1   9   4    0    0  ...     0     0     0     0   \n",
      "4205  46   1   9   3   3   1   9  24    0    0  ...     0     1     0     0   \n",
      "4206  51  23  19   5   3   1   3  22    0    0  ...     0     0     0     0   \n",
      "4207  10  23  19   0   3   1   2  16    0    0  ...     0     0     1     0   \n",
      "4208  46   1   9   2   3   1   6  17    0    0  ...     1     0     0     0   \n",
      "\n",
      "      X379  X380  X382  X383  X384  X385  \n",
      "0        0     0     0     0     0     0  \n",
      "1        0     0     0     0     0     0  \n",
      "2        0     0     1     0     0     0  \n",
      "3        0     0     0     0     0     0  \n",
      "4        0     0     0     0     0     0  \n",
      "...    ...   ...   ...   ...   ...   ...  \n",
      "4204     0     0     0     0     0     0  \n",
      "4205     0     0     0     0     0     0  \n",
      "4206     0     0     0     0     0     0  \n",
      "4207     0     0     0     0     0     0  \n",
      "4208     0     0     0     0     0     0  \n",
      "\n",
      "[8418 rows x 364 columns] \n",
      "\n",
      "\n",
      "\n",
      "Train Dataframe after applying Label Encoder\n",
      "\n",
      "      X0  X1  X2  X3  X4  X5  X6  X8  X10  X12  ...  X375  X376  X377  X378  \\\n",
      "0     37  23  20   0   3  27   9  14    0    0  ...     0     0     1     0   \n",
      "1     37  21  22   4   3  31  11  14    0    0  ...     1     0     0     0   \n",
      "2     24  24  38   2   3  30   9  23    0    0  ...     0     0     0     0   \n",
      "3     24  21  38   5   3  30  11   4    0    0  ...     0     0     0     0   \n",
      "4     24  23  38   5   3  14   3  13    0    0  ...     0     0     0     0   \n",
      "...   ..  ..  ..  ..  ..  ..  ..  ..  ...  ...  ...   ...   ...   ...   ...   \n",
      "4204  10  20  19   2   3   1   3  16    0    0  ...     1     0     0     0   \n",
      "4205  36  16  44   3   3   1   7   7    0    0  ...     0     1     0     0   \n",
      "4206  10  23  42   0   3   1   6   4    0    1  ...     0     0     1     0   \n",
      "4207  11  19  29   5   3   1  11  20    0    0  ...     0     0     0     0   \n",
      "4208  52  19   5   2   3   1   6  22    0    0  ...     1     0     0     0   \n",
      "\n",
      "      X379  X380  X382  X383  X384  X385  \n",
      "0        0     0     0     0     0     0  \n",
      "1        0     0     0     0     0     0  \n",
      "2        0     0     1     0     0     0  \n",
      "3        0     0     0     0     0     0  \n",
      "4        0     0     0     0     0     0  \n",
      "...    ...   ...   ...   ...   ...   ...  \n",
      "4204     0     0     0     0     0     0  \n",
      "4205     0     0     0     0     0     0  \n",
      "4206     0     0     0     0     0     0  \n",
      "4207     0     0     0     0     0     0  \n",
      "4208     0     0     0     0     0     0  \n",
      "\n",
      "[4209 rows x 364 columns] \n",
      "\n",
      "\n",
      "\n",
      "Test Dataframe after applying Label Encoder\n",
      "\n",
      "      X0  X1  X2  X3  X4  X5  X6  X8  X10  X12  ...  X375  X376  X377  X378  \\\n",
      "0     24  23  38   5   3  26   0  22    0    0  ...     0     0     0     1   \n",
      "1     46   3   9   0   3   9   6  24    0    0  ...     0     0     1     0   \n",
      "2     24  23  19   5   3   0   9   9    0    0  ...     0     0     0     1   \n",
      "3     24  13  38   5   3  32  11  13    0    0  ...     0     0     0     1   \n",
      "4     49  20  19   2   3  31   8  12    0    0  ...     1     0     0     0   \n",
      "...   ..  ..  ..  ..  ..  ..  ..  ..  ...  ...  ...   ...   ...   ...   ...   \n",
      "4204   9   9  19   5   3   1   9   4    0    0  ...     0     0     0     0   \n",
      "4205  46   1   9   3   3   1   9  24    0    0  ...     0     1     0     0   \n",
      "4206  51  23  19   5   3   1   3  22    0    0  ...     0     0     0     0   \n",
      "4207  10  23  19   0   3   1   2  16    0    0  ...     0     0     1     0   \n",
      "4208  46   1   9   2   3   1   6  17    0    0  ...     1     0     0     0   \n",
      "\n",
      "      X379  X380  X382  X383  X384  X385  \n",
      "0        0     0     0     0     0     0  \n",
      "1        0     0     0     0     0     0  \n",
      "2        0     0     0     0     0     0  \n",
      "3        0     0     0     0     0     0  \n",
      "4        0     0     0     0     0     0  \n",
      "...    ...   ...   ...   ...   ...   ...  \n",
      "4204     0     0     0     0     0     0  \n",
      "4205     0     0     0     0     0     0  \n",
      "4206     0     0     0     0     0     0  \n",
      "4207     0     0     0     0     0     0  \n",
      "4208     0     0     0     0     0     0  \n",
      "\n",
      "[4209 rows x 364 columns] \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Dataset with Standard Scaler : \n",
      "\n",
      "(8418, 364) \n",
      "\n",
      "\n",
      "\n",
      "Explained variance ratio of Scaled Dataset \n",
      "\n",
      "[6.96235499e-02 5.61572698e-02 4.63808371e-02 3.43122001e-02\n",
      " 3.24041302e-02 3.14465839e-02 2.82590702e-02 2.11608931e-02\n",
      " 1.92092270e-02 1.73092528e-02 1.63288118e-02 1.59575381e-02\n",
      " 1.46133192e-02 1.38858481e-02 1.32424174e-02 1.23871474e-02\n",
      " 1.23418785e-02 1.15384889e-02 1.12730650e-02 1.08665066e-02\n",
      " 9.97892723e-03 9.32495675e-03 9.31301192e-03 9.04861164e-03\n",
      " 8.64015566e-03 8.12361434e-03 7.38541054e-03 7.37046756e-03\n",
      " 7.30015139e-03 6.96827188e-03 6.69562827e-03 6.62245755e-03\n",
      " 6.43082374e-03 6.26133328e-03 6.15169810e-03 6.06887290e-03\n",
      " 5.88604306e-03 5.76172595e-03 5.59895518e-03 5.53115304e-03\n",
      " 5.38447643e-03 5.32403293e-03 5.16816623e-03 5.13032768e-03\n",
      " 5.03658780e-03 4.95440550e-03 4.88788816e-03 4.79668755e-03\n",
      " 4.59349607e-03 4.57063671e-03 4.40552332e-03 4.32515874e-03\n",
      " 4.28000521e-03 4.19301704e-03 4.18183215e-03 4.06874556e-03\n",
      " 3.99262160e-03 3.90760865e-03 3.83368623e-03 3.77491969e-03\n",
      " 3.71427050e-03 3.64448633e-03 3.63041471e-03 3.57464166e-03\n",
      " 3.56456358e-03 3.53109322e-03 3.47491336e-03 3.38984137e-03\n",
      " 3.33094673e-03 3.32146308e-03 3.28570562e-03 3.22991664e-03\n",
      " 3.19952681e-03 3.17175719e-03 3.14589357e-03 3.10605133e-03\n",
      " 3.07500715e-03 3.05208371e-03 3.03849715e-03 3.00513552e-03\n",
      " 2.99402924e-03 2.96092763e-03 2.95390537e-03 2.92977461e-03\n",
      " 2.91643530e-03 2.89377938e-03 2.88055565e-03 2.86761340e-03\n",
      " 2.82919410e-03 2.78190012e-03 2.77563980e-03 2.75545762e-03\n",
      " 2.75013415e-03 2.73278325e-03 2.68975639e-03 2.68223333e-03\n",
      " 2.66850432e-03 2.64746406e-03 2.63614619e-03 2.59600396e-03\n",
      " 2.58830928e-03 2.55712759e-03 2.54941569e-03 2.50976028e-03\n",
      " 2.47657197e-03 2.46231766e-03 2.45742580e-03 2.43488478e-03\n",
      " 2.42447719e-03 2.39752859e-03 2.37958813e-03 2.36777150e-03\n",
      " 2.35078193e-03 2.31169412e-03 2.29336681e-03 2.26464931e-03\n",
      " 2.24021740e-03 2.21606786e-03 2.20458822e-03 2.18412167e-03\n",
      " 2.16272146e-03 2.11402513e-03 2.08718339e-03 2.06362892e-03\n",
      " 2.06135588e-03 2.04618997e-03 2.02593478e-03 1.99781850e-03\n",
      " 1.97719476e-03 1.96217648e-03 1.93994495e-03 1.92070943e-03\n",
      " 1.87756320e-03 1.86048339e-03 1.81450494e-03 1.80131232e-03\n",
      " 1.76394295e-03 1.75652490e-03 1.73406016e-03 1.72745110e-03\n",
      " 1.65874106e-03 1.65349250e-03 1.63495356e-03 1.59279342e-03\n",
      " 1.56501483e-03 1.54417159e-03 1.52019038e-03 1.43105348e-03\n",
      " 1.40700655e-03 1.39700477e-03 1.34805337e-03 1.32395968e-03\n",
      " 1.28558622e-03 1.26614200e-03 1.22896724e-03 1.20455692e-03\n",
      " 1.20344745e-03 1.18300056e-03 1.15591401e-03 1.13627471e-03\n",
      " 1.12465250e-03 1.09221194e-03 1.06983205e-03 1.03305377e-03\n",
      " 1.00207385e-03 9.80417090e-04 9.69559327e-04 9.27223959e-04\n",
      " 9.12933907e-04 9.06435284e-04 8.86633557e-04 8.73582939e-04\n",
      " 8.57698756e-04 8.19898545e-04 8.08761553e-04 7.92571657e-04\n",
      " 7.73508099e-04 7.63411873e-04 7.39213417e-04 7.13134881e-04\n",
      " 6.87613749e-04 6.77884483e-04 6.56780860e-04 6.24995633e-04\n",
      " 6.14738849e-04 5.88557669e-04 5.80413471e-04 5.69705930e-04\n",
      " 5.53520578e-04 5.37192237e-04 5.28439783e-04 5.16202030e-04\n",
      " 5.01959337e-04 4.85211238e-04 4.81888023e-04 4.78501555e-04\n",
      " 4.65410459e-04 4.50461737e-04 4.40943564e-04 4.27690560e-04\n",
      " 4.09305779e-04 4.03158117e-04 3.96284300e-04 3.92060420e-04\n",
      " 3.88554935e-04 3.68997813e-04 3.58577580e-04 3.46033818e-04\n",
      " 3.41817417e-04 3.37339046e-04 3.25933880e-04 3.14874990e-04\n",
      " 3.00233194e-04 2.79439915e-04 2.75502974e-04 2.69475169e-04\n",
      " 2.59940220e-04 2.56351983e-04 2.52869463e-04 2.44474512e-04\n",
      " 2.32658389e-04 2.13488944e-04 2.03995153e-04 1.97580022e-04\n",
      " 1.86373781e-04 1.78047363e-04 1.73043641e-04 1.69208880e-04\n",
      " 1.64915744e-04 1.60788065e-04 1.45396142e-04 1.41595309e-04\n",
      " 1.34537685e-04 1.26075396e-04 1.19202231e-04 1.13019004e-04\n",
      " 1.05674283e-04 9.77549594e-05 8.55786413e-05 7.95106643e-05\n",
      " 7.88753124e-05 7.21744161e-05 6.99835068e-05 6.72955467e-05\n",
      " 6.13183234e-05 5.77830123e-05 5.34717696e-05 4.95535934e-05\n",
      " 4.89297157e-05 4.63514398e-05 4.51214744e-05 4.14095643e-05\n",
      " 3.90245458e-05 3.22314797e-05 3.07010213e-05 2.76989236e-05\n",
      " 2.22711326e-05 2.12295058e-05 1.97317790e-05 1.66301243e-05\n",
      " 1.65307256e-05 1.41800778e-05 1.29642066e-05 1.20955677e-05\n",
      " 9.87999729e-06 8.99758663e-06 8.49630432e-06 8.10539891e-06\n",
      " 5.78711653e-06 5.24569752e-06 5.05376749e-06 4.04380755e-06\n",
      " 4.01141465e-06 3.62649933e-06 3.16163999e-06 2.53411720e-06\n",
      " 1.99401296e-06 1.20576219e-06 1.02694608e-06 3.90362783e-07\n",
      " 1.37308242e-32 1.74372756e-33 1.45703810e-33 1.29222709e-33\n",
      " 1.02904817e-33 1.02741369e-33 8.45803750e-34 7.63794819e-34\n",
      " 4.97677871e-34 4.29917031e-34 3.21672512e-34 3.21672512e-34\n",
      " 3.21672512e-34 3.21672512e-34 3.21672512e-34 3.21672512e-34\n",
      " 3.21672512e-34 3.21672512e-34 3.21672512e-34 3.21672512e-34\n",
      " 3.21672512e-34 3.21672512e-34 3.21672512e-34 3.21672512e-34\n",
      " 3.21672512e-34 3.21672512e-34 3.21672512e-34 3.21672512e-34\n",
      " 3.21672512e-34 3.21672512e-34 3.21672512e-34 3.21672512e-34\n",
      " 3.21672512e-34 3.21672512e-34 3.21672512e-34 3.21672512e-34\n",
      " 3.21672512e-34 3.21672512e-34 3.21672512e-34 3.21672512e-34\n",
      " 3.21672512e-34 3.21672512e-34 3.21672512e-34 3.21672512e-34\n",
      " 3.21672512e-34 3.21672512e-34 3.21672512e-34 3.21672512e-34\n",
      " 3.21672512e-34 3.21672512e-34 3.21672512e-34 3.21672512e-34\n",
      " 3.21672512e-34 3.21672512e-34 3.21672512e-34 3.21672512e-34\n",
      " 3.21672512e-34 3.21672512e-34 3.21672512e-34 3.21672512e-34\n",
      " 3.21672512e-34 3.21672512e-34 3.21672512e-34 3.21672512e-34\n",
      " 3.21672512e-34 3.21672512e-34 3.21672512e-34 3.21672512e-34\n",
      " 3.21672512e-34 3.21672512e-34 3.21672512e-34 3.21672512e-34\n",
      " 3.21672512e-34 3.21672512e-34 3.21672512e-34 3.21672512e-34\n",
      " 3.21672512e-34 3.21672512e-34 3.21672512e-34 2.65775202e-34\n",
      " 1.22337132e-34 1.06145954e-34 1.05677628e-34 1.05886637e-35] \n",
      "\n",
      "\n",
      "\n",
      "Graph for variance ratio : \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None \n",
      "\n",
      "\n",
      "\n",
      "Explained variance ratio of Scaled Dataset with cumulative sum : \n",
      "\n",
      "[0.06962355 0.12578082 0.17216166 0.20647386 0.23887799 0.27032457\n",
      " 0.29858364 0.31974453 0.33895376 0.35626301 0.37259183 0.38854936\n",
      " 0.40316268 0.41704853 0.43029095 0.4426781  0.45501997 0.46655846\n",
      " 0.47783153 0.48869804 0.49867696 0.50800192 0.51731493 0.52636354\n",
      " 0.5350037  0.54312731 0.55051272 0.55788319 0.56518334 0.57215161\n",
      " 0.57884724 0.5854697  0.59190052 0.59816186 0.60431356 0.61038243\n",
      " 0.61626847 0.6220302  0.62762915 0.63316031 0.63854478 0.64386881\n",
      " 0.64903698 0.65416731 0.6592039  0.6641583  0.66904619 0.67384288\n",
      " 0.67843637 0.68300701 0.68741253 0.69173769 0.6960177  0.70021071\n",
      " 0.70439255 0.70846129 0.71245391 0.71636152 0.72019521 0.72397013\n",
      " 0.7276844  0.73132889 0.7349593  0.73853394 0.74209851 0.7456296\n",
      " 0.74910451 0.75249435 0.7558253  0.75914676 0.76243247 0.76566239\n",
      " 0.76886191 0.77203367 0.77517956 0.77828561 0.78136062 0.78441271\n",
      " 0.7874512  0.79045634 0.79345037 0.79641129 0.7993652  0.80229497\n",
      " 0.80521141 0.80810519 0.81098574 0.81385336 0.81668255 0.81946445\n",
      " 0.82224009 0.82499555 0.82774568 0.83047847 0.83316822 0.83585046\n",
      " 0.83851896 0.84116643 0.84380257 0.84639858 0.84898688 0.85154401\n",
      " 0.85409343 0.85660319 0.85907976 0.86154208 0.8639995  0.86643439\n",
      " 0.86885887 0.87125639 0.87363598 0.87600375 0.87835454 0.88066623\n",
      " 0.8829596  0.88522425 0.88746446 0.88968053 0.89188512 0.89406924\n",
      " 0.89623196 0.89834599 0.90043317 0.9024968  0.90455816 0.90660435\n",
      " 0.90863028 0.9106281  0.91260529 0.91456747 0.91650742 0.91842813\n",
      " 0.92030569 0.92216617 0.92398068 0.92578199 0.92754593 0.92930246\n",
      " 0.93103652 0.93276397 0.93442271 0.9360762  0.93771116 0.93930395\n",
      " 0.94086896 0.94241313 0.94393333 0.94536438 0.94677139 0.94816839\n",
      " 0.94951644 0.9508404  0.95212599 0.95339213 0.9546211  0.95582566\n",
      " 0.9570291  0.9582121  0.95936802 0.96050429 0.96162894 0.96272116\n",
      " 0.96379099 0.96482404 0.96582612 0.96680653 0.96777609 0.96870332\n",
      " 0.96961625 0.97052269 0.97140932 0.9722829  0.9731406  0.9739605\n",
      " 0.97476926 0.97556183 0.97633534 0.97709875 0.97783797 0.9785511\n",
      " 0.97923871 0.9799166  0.98057338 0.98119838 0.98181311 0.98240167\n",
      " 0.98298209 0.98355179 0.98410531 0.9846425  0.98517094 0.98568715\n",
      " 0.98618911 0.98667432 0.98715621 0.98763471 0.98810012 0.98855058\n",
      " 0.98899152 0.98941921 0.98982852 0.99023168 0.99062796 0.99102002\n",
      " 0.99140858 0.99177757 0.99213615 0.99248219 0.992824   0.99316134\n",
      " 0.99348728 0.99380215 0.99410238 0.99438182 0.99465733 0.9949268\n",
      " 0.99518674 0.99544309 0.99569596 0.99594044 0.9961731  0.99638659\n",
      " 0.99659058 0.99678816 0.99697453 0.99715258 0.99732563 0.99749483\n",
      " 0.99765975 0.99782054 0.99796593 0.99810753 0.99824207 0.99836814\n",
      " 0.99848735 0.99860036 0.99870604 0.99880379 0.99888937 0.99896888\n",
      " 0.99904776 0.99911993 0.99918992 0.99925721 0.99931853 0.99937631\n",
      " 0.99942978 0.99947934 0.99952827 0.99957462 0.99961974 0.99966115\n",
      " 0.99970018 0.99973241 0.99976311 0.99979081 0.99981308 0.99983431\n",
      " 0.99985404 0.99987067 0.9998872  0.99990138 0.99991434 0.99992644\n",
      " 0.99993632 0.99994532 0.99995381 0.99996192 0.99996771 0.99997295\n",
      " 0.99997801 0.99998205 0.99998606 0.99998969 0.99999285 0.99999538\n",
      " 0.99999738 0.99999858 0.99999961 1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.        ] \n",
      "\n",
      "\n",
      "\n",
      "Graph for Cumulative sum of varaince ratio : \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None \n",
      "\n",
      "\n",
      "\n",
      "Shape of Dataset after dimensionality reduction : \n",
      "\n",
      "(8418, 123) \n",
      "\n",
      "\n",
      "\n",
      "Dataset after dimensionality reduction : \n",
      "\n",
      "            PC0        PC1       PC2       PC3       PC4       PC5       PC6  \\\n",
      "0     11.580577  -1.908280 -1.645008  1.290739 -0.907196 -2.328201  7.854727   \n",
      "1     -0.133096   0.353761  0.829697  1.140162 -2.406473  0.347280  1.021301   \n",
      "2     10.451144  21.494189 -6.071369 -3.742154  1.394463  2.533316  0.762914   \n",
      "3      7.349810  21.238342 -6.796905  0.736425  2.141557 -0.143797  0.289508   \n",
      "4      6.629969  21.411149 -7.356675  1.441374  2.137933 -0.885920 -1.016445   \n",
      "...         ...        ...       ...       ...       ...       ...       ...   \n",
      "8413  -1.526370   1.537673  5.776801 -0.220262 -0.976992 -1.134596 -3.433141   \n",
      "8414   0.088926  -2.042986 -4.042179  0.574480 -2.544394  1.791152  3.068706   \n",
      "8415  -2.664512   0.808127  3.117285  2.890138 -0.462366 -1.568877 -3.866196   \n",
      "8416  -1.254102   0.469456  4.182495 -4.459709  0.060282 -0.463933  1.427608   \n",
      "8417  -1.463493  -2.093108 -3.445282  2.440254 -4.823620  2.691580  0.981778   \n",
      "\n",
      "           PC7        PC8       PC9  ...     PC113     PC114     PC115  \\\n",
      "0    -1.950396 -13.069384 -1.048823  ... -0.560770 -0.658221 -0.247455   \n",
      "1    -0.771919  -0.383078  0.014709  ...  0.061214 -0.117142 -0.776560   \n",
      "2     3.523860  -0.987806 -0.819300  ... -2.714473  1.614915 -1.376188   \n",
      "3    -0.537676   0.503757 -3.957188  ...  1.383613 -0.323443 -1.108093   \n",
      "4    -0.879974   1.318950 -1.672020  ... -1.271267  1.270644 -1.278067   \n",
      "...        ...        ...       ...  ...       ...       ...       ...   \n",
      "8413  0.499018   0.501311  2.096024  ...  0.339813  0.051889  0.236492   \n",
      "8414  3.343465  -2.472185  1.287871  ... -0.765019  0.575813  0.594071   \n",
      "8415  1.073500  -0.146038  0.831529  ...  1.262324 -1.478074  0.315519   \n",
      "8416  0.607461   0.822807  0.757675  ... -0.432381  0.289996 -1.476170   \n",
      "8417  4.278793   0.003636 -1.625849  ...  0.247764 -0.185627  0.273550   \n",
      "\n",
      "         PC116     PC117     PC118     PC119     PC120     PC121     PC122  \n",
      "0     0.513131 -0.592175  1.217348 -0.341925 -0.779232 -0.368804  0.433686  \n",
      "1     1.004506 -0.995470  1.431915 -0.118116 -0.770408  0.003983 -2.348176  \n",
      "2    -3.176163 -5.381098  1.845317  6.552925 -6.922002 -1.694684 -7.779917  \n",
      "3    -0.079159 -0.464203  0.910741 -0.521385  0.438168 -0.683763  0.700810  \n",
      "4    -3.053061 -5.103573  1.769064  4.230389 -2.951015 -1.260393 -6.785916  \n",
      "...        ...       ...       ...       ...       ...       ...       ...  \n",
      "8413  0.601017  0.517634  0.416976  0.112931  0.512860  0.147691 -1.712074  \n",
      "8414 -0.238308 -0.324755 -1.034982  0.431992 -1.788777 -1.551038 -0.872496  \n",
      "8415 -0.555632  1.356840 -0.838052  0.598379 -0.156964 -0.526858 -0.690382  \n",
      "8416 -0.045491  0.434734 -0.971413 -0.019800  0.193165  0.290545  0.428145  \n",
      "8417 -0.063707  0.173325 -0.207918  0.653677 -0.046337 -0.128869 -0.247337  \n",
      "\n",
      "[8418 rows x 123 columns] \n",
      "\n",
      "\n",
      "\n",
      "Train acc :  0.9558843695157845 \n",
      "\n",
      "Predicted target variable using test_df :  [ 73.84491  90.63363  81.8471  ...  98.09098 109.03973  92.53824]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#Loading Datasets\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print('Training Info : \\n')\n",
    "print(train_df.info(), '\\n\\n\\n')\n",
    "print('Training Data : \\n')\n",
    "print(train_df.head(),'\\n\\n\\n')\n",
    "print('Test Data : \\n')\n",
    "print(test_df.head(), '\\n\\n\\n')\n",
    "print('Train Dataframe shape : \\n')\n",
    "print(train_df.shape,'\\n\\n\\n')\n",
    "print('Test Dataframe shape : \\n')\n",
    "print(test_df.shape,'\\n\\n\\n')\n",
    "\n",
    "train_y = train_df['y']\n",
    "train_df = train_df.drop(['ID', 'y'], axis = 1)\n",
    "test_df = test_df.drop('ID', axis = 1)\n",
    "print('Train Dataframe after dropping ID and y columns\\n')\n",
    "print(train_df.head(), '\\n\\n\\n')\n",
    "print('Test dataframe after dropping ID column\\n')\n",
    "print(test_df.head(), '\\n\\n\\n')\n",
    "## Removing variable having variance 0\n",
    "for i in train_df.columns:\n",
    "    if train_df[i].dtype != object:\n",
    "        if train_df[i].var() == 0:\n",
    "            train_df = train_df.drop(i, axis = 1)\n",
    "            test_df = test_df.drop(i, axis = 1) \n",
    "total_df = pd.concat([train_df, test_df])\n",
    "\n",
    "\n",
    "print('Train Dataframe after removing columns with varaince 0 : \\n')\n",
    "print(train_df.head(), '\\n\\n\\n')\n",
    "\n",
    "print('Test Dataframe after removing columns with varaince 0 : \\n')\n",
    "print(test_df.head(), '\\n\\n\\n')\n",
    "\n",
    "print('Dataframe after concatinating train_df and test_df\\n')\n",
    "print(total_df, '\\n\\n\\n')\n",
    "\n",
    "\n",
    "## Check for null and unique values for test and train sets.\n",
    "print('Check null values for train sets. \\n')\n",
    "print(train_df.columns[train_df.isnull().any()])\n",
    "print(train_df[train_df.isnull().any(axis = 1)], '\\n\\n\\n')\n",
    "\n",
    "print('Check null values for test sets.\\n')\n",
    "print(test_df.columns[test_df.isnull().any()])\n",
    "print(test_df[test_df.isnull().any(axis = 1)],'\\n\\n\\n')\n",
    "\n",
    "## Apply label encoder.\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "for i in total_df.columns:\n",
    "    if total_df[i].dtype == object:\n",
    "        total_df[i] = le.fit_transform(total_df[i])\n",
    "\n",
    "print('Total Dataframe after applying Label Encoder : \\n')\n",
    "print(total_df, '\\n\\n\\n')\n",
    "\n",
    "train_df = total_df[:len(train_df)]\n",
    "test_df = total_df[len(train_df):]\n",
    "\n",
    "print('Train Dataframe after applying Label Encoder\\n')\n",
    "print(train_df, '\\n\\n\\n')\n",
    "\n",
    "print('Test Dataframe after applying Label Encoder\\n')\n",
    "print(test_df, '\\n\\n\\n')\n",
    "\n",
    "## Dimensonality reduction\n",
    "\n",
    "## Using PCA for dimensionality reduction \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(total_df)\n",
    "\n",
    "print('Scaled Dataset with Standard Scaler : \\n')\n",
    "print(X_scaled.shape, '\\n\\n\\n')\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "print('Explained variance ratio of Scaled Dataset \\n')\n",
    "print(pca.explained_variance_ratio_, '\\n\\n\\n')\n",
    "print('Graph for variance ratio : \\n')\n",
    "plt.plot(['PC'+ str(i) for i in range(364)],pca.explained_variance_ratio_)\n",
    "print(plt.show(), '\\n\\n\\n')\n",
    "print('Explained variance ratio of Scaled Dataset with cumulative sum : \\n')\n",
    "print(pca.explained_variance_ratio_.cumsum(), '\\n\\n\\n')\n",
    "\n",
    "print('Graph for Cumulative sum of varaince ratio : \\n')\n",
    "plt.bar(['PC'+ str(i) for i in range(364)],pca.explained_variance_ratio_.cumsum())\n",
    "print(plt.show(), '\\n\\n\\n')\n",
    "pca = PCA(n_components = 0.90)\n",
    "\n",
    "PCA_X = pca.fit_transform(X_scaled)\n",
    "\n",
    "print('Shape of Dataset after dimensionality reduction : \\n')\n",
    "print(PCA_X.shape, '\\n\\n\\n')\n",
    "\n",
    "PCA_df = pd.DataFrame(PCA_X, columns = ['PC' + str(i) for i in range(123)])\n",
    "print('Dataset after dimensionality reduction : \\n')\n",
    "print(PCA_df, '\\n\\n\\n')\n",
    "train_df = PCA_X[:len(train_df)]\n",
    "test_df = PCA_X[len(train_df):]\n",
    "\n",
    "\n",
    "##  Applying XGBoost algorithm \n",
    "from xgboost import XGBRegressor\n",
    "xgb = XGBRegressor()\n",
    "xgb.fit(train_df, train_y)\n",
    "print('Train acc : ', xgb.score(train_df, train_y), '\\n')\n",
    "predected_y = xgb.predict(test_df)\n",
    "print('Predicted target variable using test_df : ',predected_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afd9836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#Loading Datasets\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "print('Training Info : \\n')\n",
    "print(train_df.info(), '\\n\\n\\n')\n",
    "print('Training Data : \\n')\n",
    "print(train_df.head(),'\\n\\n\\n')\n",
    "print('Test Data : \\n')\n",
    "print(test_df.head(), '\\n\\n\\n')\n",
    "print('Train Dataframe shape : \\n')\n",
    "print(train_df.shape,'\\n\\n\\n')\n",
    "print('Test Dataframe shape : \\n')\n",
    "print(test_df.shape,'\\n\\n\\n')\n",
    "train_y = train_df['y']\n",
    "train_df = train_df.drop(['ID', 'y'], axis = 1)\n",
    "test_df = test_df.drop('ID', axis = 1)\n",
    "print('Train Dataframe after dropping ID and y columns\\n')\n",
    "print(train_df.head(), '\\n\\n\\n')\n",
    "print('Test dataframe after dropping ID column\\n')\n",
    "print(test_df.head(), '\\n\\n\\n')\n",
    "## Removing variable having variance 0\n",
    "for i in train_df.columns:\n",
    " if train_df[i].dtype != object:\n",
    " if train_df[i].var() == 0:\n",
    " train_df = train_df.drop(i, axis = 1)\n",
    " test_df = test_df.drop(i, axis = 1) \n",
    "total_df = pd.concat([train_df, test_df])\n",
    "print('Train Dataframe after removing columns with varaince 0 : \\n')\n",
    "print(train_df.head(), '\\n\\n\\n')\n",
    "print('Test Dataframe after removing columns with varaince 0 : \\n')\n",
    "print(test_df.head(), '\\n\\n\\n')\n",
    "print('Dataframe after concatinating train_df and test_df\\n')\n",
    "print(total_df, '\\n\\n\\n')\n",
    "## Check for null and unique values for test and train sets.\n",
    "print('Check null values for train sets. \\n')\n",
    "print(train_df.columns[train_df.isnull().any()])\n",
    "print(train_df[train_df.isnull().any(axis = 1)], '\\n\\n\\n')\n",
    "print('Check null values for test sets.\\n')\n",
    "print(test_df.columns[test_df.isnull().any()])\n",
    "print(test_df[test_df.isnull().any(axis = 1)],'\\n\\n\\n')\n",
    "## Apply label encoder.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "for i in total_df.columns:\n",
    " if total_df[i].dtype == object:\n",
    " total_df[i] = le.fit_transform(total_df[i])\n",
    "print('Total Dataframe after applying Label Encoder : \\n')\n",
    "print(total_df, '\\n\\n\\n')\n",
    "train_df = total_df[:len(train_df)]\n",
    "test_df = total_df[len(train_df):]\n",
    "print('Train Dataframe after applying Label Encoder\\n')\n",
    "print(train_df, '\\n\\n\\n')\n",
    "print('Test Dataframe after applying Label Encoder\\n')\n",
    "print(test_df, '\\n\\n\\n')\n",
    "## Dimensonality reduction\n",
    "## Using PCA for dimensionality reduction \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(total_df)\n",
    "print('Scaled Dataset with Standard Scaler : \\n')\n",
    "print(X_scaled.shape, '\\n\\n\\n')\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "print('Explained variance ratio of Scaled Dataset \\n')\n",
    "print(pca.explained_variance_ratio_, '\\n\\n\\n')\n",
    "print('Graph for variance ratio : \\n')\n",
    "plt.plot(['PC'+ str(i) for i in range(364)],pca.explained_variance_ratio_)\n",
    "print(plt.show(), '\\n\\n\\n')\n",
    "print('Explained variance ratio of Scaled Dataset with cumulative sum : \\n')\n",
    "print(pca.explained_variance_ratio_.cumsum(), '\\n\\n\\n')\n",
    "print('Graph for Cumulative sum of varaince ratio : \\n')\n",
    "plt.bar(['PC'+ str(i) for i in range(364)],pca.explained_variance_ratio_.cumsum())\n",
    "print(plt.show(), '\\n\\n\\n')\n",
    "pca = PCA(n_components = 0.90)\n",
    "PCA_X = pca.fit_transform(X_scaled)\n",
    "print('Shape of Dataset after dimensionality reduction : \\n')\n",
    "print(PCA_X.shape, '\\n\\n\\n')\n",
    "PCA_df = pd.DataFrame(PCA_X, columns = ['PC' + str(i) for i in range(123)])\n",
    "print('Dataset after dimensionality reduction : \\n')\n",
    "print(PCA_df, '\\n\\n\\n')\n",
    "train_df = PCA_X[:len(train_df)]\n",
    "test_df = PCA_X[len(train_df):]\n",
    "## Applying XGBoost algorithm \n",
    "from xgboost import XGBRegressor\n",
    "xgb = XGBRegressor()\n",
    "xgb.fit(train_df, train_y)\n",
    "print('Train acc : ', xgb.score(train_df, train_y), '\\n')\n",
    "predected_y = xgb.predict(test_df)\n",
    "print('Predicted target variable using test_df : ',predected_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeeeaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Source code :\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#Loading Datasets\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print('Training Info : \\n')\n",
    "print(train_df.info(), '\\n\\n\\n')\n",
    "\n",
    "print('Training Data : \\n')\n",
    "print(train_df.head(),'\\n\\n\\n')\n",
    "\n",
    "print('Test Data : \\n')\n",
    "print(test_df.head(), '\\n\\n\\n')\n",
    "\n",
    "print('Train Dataframe shape : \\n')\n",
    "print(train_df.shape,'\\n\\n\\n')\n",
    "print('Test Dataframe shape : \\n')\n",
    "print(test_df.shape,'\\n\\n\\n')\n",
    "\n",
    "train_y = train_df['y']\n",
    "train_df = train_df.drop(['ID', 'y'], axis = 1)\n",
    "test_df = test_df.drop('ID', axis = 1)\n",
    "\n",
    "print('Train Dataframe after dropping ID and y columns\\n')\n",
    "print(train_df.head(), '\\n\\n\\n')\n",
    "print('Test dataframe after dropping ID column\\n')\n",
    "print(test_df.head(), '\\n\\n\\n')\n",
    "\n",
    "\n",
    "## Removing variable having variance 0\n",
    "\n",
    "for i in train_df.columns:\n",
    "    if train_df[i].dtype != object:\n",
    "        if train_df[i].var() == 0:\n",
    "            train_df = train_df.drop(i, axis = 1)\n",
    "            test_df = test_df.drop(i, axis = 1) \n",
    "total_df = pd.concat([train_df, test_df])\n",
    "\n",
    "\n",
    "print('Train Dataframe after removing columns with varaince 0 : \\n')\n",
    "print(train_df.head(), '\\n\\n\\n')\n",
    "\n",
    "print('Test Dataframe after removing columns with varaince 0 : \\n')\n",
    "print(test_df.head(), '\\n\\n\\n')\n",
    "\n",
    "print('Dataframe after concatinating train_df and test_df\\n')\n",
    "print(total_df, '\\n\\n\\n')\n",
    "\n",
    "\n",
    "## Check for null and unique values for test and train sets.\n",
    "print('Check null values for train sets. \\n')\n",
    "print(train_df.columns[train_df.isnull().any()])\n",
    "print(train_df[train_df.isnull().any(axis = 1)], '\\n\\n\\n')\n",
    "\n",
    "print('Check null values for test sets.\\n')\n",
    "print(test_df.columns[test_df.isnull().any()])\n",
    "print(test_df[test_df.isnull().any(axis = 1)],'\\n\\n\\n')\n",
    "\n",
    "## Apply label encoder.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "for i in total_df.columns:\n",
    "    if total_df[i].dtype == object:\n",
    "        total_df[i] = le.fit_transform(total_df[i])\n",
    "\n",
    "print('Total Dataframe after applying Label Encoder : \\n')\n",
    "print(total_df, '\\n\\n\\n')\n",
    "\n",
    "train_df = total_df[:len(train_df)]\n",
    "test_df = total_df[len(train_df):]\n",
    "\n",
    "print('Train Dataframe after applying Label Encoder\\n')\n",
    "print(train_df, '\\n\\n\\n')\n",
    "\n",
    "print('Test Dataframe after applying Label Encoder\\n')\n",
    "print(test_df, '\\n\\n\\n')\n",
    "\n",
    "## Dimensonality reduction\n",
    "\n",
    "## Using PCA for dimensionality reduction \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(total_df)\n",
    "\n",
    "print('Scaled Dataset with Standard Scaler : \\n')\n",
    "print(X_scaled.shape, '\\n\\n\\n')\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "print('Explained variance ratio of Scaled Dataset \\n')\n",
    "print(pca.explained_variance_ratio_, '\\n\\n\\n')\n",
    "print('Graph for variance ratio : \\n')\n",
    "plt.plot(['PC'+ str(i) for i in range(364)],pca.explained_variance_ratio_)\n",
    "print(plt.show(), '\\n\\n\\n')\n",
    "print('Explained variance ratio of Scaled Dataset with cumulative sum : \\n')\n",
    "print(pca.explained_variance_ratio_.cumsum(), '\\n\\n\\n')\n",
    "\n",
    "print('Graph for Cumulative sum of varaince ratio : \\n')\n",
    "plt.bar(['PC'+ str(i) for i in range(364)],pca.explained_variance_ratio_.cumsum())\n",
    "print(plt.show(), '\\n\\n\\n')\n",
    "pca = PCA(n_components = 0.90)\n",
    "\n",
    "PCA_X = pca.fit_transform(X_scaled)\n",
    "\n",
    "print('Shape of Dataset after dimensionality reduction : \\n')\n",
    "print(PCA_X.shape, '\\n\\n\\n')\n",
    "\n",
    "PCA_df = pd.DataFrame(PCA_X, columns = ['PC' + str(i) for i in range(123)])\n",
    "print('Dataset after dimensionality reduction : \\n')\n",
    "print(PCA_df, '\\n\\n\\n')\n",
    "\n",
    "train_df = PCA_X[:len(train_df)]\n",
    "test_df = PCA_X[len(train_df):]\n",
    "\n",
    "\n",
    "##  Applying XGBoost algorithm \n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "xgb = XGBRegressor()\n",
    "xgb.fit(train_df, train_y)\n",
    "print('Train acc : ', xgb.score(train_df, train_y), '\\n')\n",
    "predected_y = xgb.predict(test_df)\n",
    "print('Predicted target variable using test_df : ',predected_y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
